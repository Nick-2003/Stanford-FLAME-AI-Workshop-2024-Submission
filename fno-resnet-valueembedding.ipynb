{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-17T21:54:09.047594Z",
     "iopub.status.busy": "2024-10-17T21:54:09.046505Z",
     "iopub.status.idle": "2024-10-17T21:54:09.053296Z",
     "shell.execute_reply": "2024-10-17T21:54:09.052201Z",
     "shell.execute_reply.started": "2024-10-17T21:54:09.047552Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T21:54:13.572623Z",
     "iopub.status.busy": "2024-10-17T21:54:13.571919Z",
     "iopub.status.idle": "2024-10-17T21:54:13.636311Z",
     "shell.execute_reply": "2024-10-17T21:54:13.635348Z",
     "shell.execute_reply.started": "2024-10-17T21:54:13.572583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "print('Device: ',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-17T21:54:21.067458Z",
     "iopub.status.busy": "2024-10-17T21:54:21.067085Z",
     "iopub.status.idle": "2024-10-17T21:54:22.964126Z",
     "shell.execute_reply": "2024-10-17T21:54:22.963103Z",
     "shell.execute_reply.started": "2024-10-17T21:54:21.067416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 150, 5, 113, 32])\n",
      "torch.Size([27, 5, 5, 113, 32])\n",
      "torch.Size([1350, 5, 113, 32]) torch.Size([135, 5, 113, 32])\n",
      "torch.Size([1, 1, 5, 1, 1])\n",
      "torch.Size([1, 1, 5, 1, 1])\n",
      "torch.Size([1, 1, 3, 1, 1])\n",
      "torch.Size([1, 1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "input_path = '/kaggle/input/2024-flame-ai-challenge/dataset/'\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv(os.path.join(input_path, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(input_path, 'test.csv'))\n",
    "\n",
    "# Function to load data\n",
    "def load_dataX(idx, df, data_dir):\n",
    "    csv_file = df.reset_index().to_dict(orient='list')\n",
    "    dir_path = os.path.join(input_path, data_dir)\n",
    "    \n",
    "    id = csv_file['id'][idx]\n",
    "    nt, Nx, Ny = csv_file['Nt'][idx], csv_file['Nx'][idx], csv_file['Ny'][idx]\n",
    "    \n",
    "    theta = np.fromfile(os.path.join(dir_path, csv_file['theta_filename'][idx]), dtype=\"<f4\").reshape(nt, Nx, Ny)\n",
    "    ustar = np.fromfile(os.path.join(dir_path, csv_file['ustar_filename'][idx]), dtype=\"<f4\").reshape(nt, Nx, Ny)\n",
    "    xi_f = np.fromfile(os.path.join(dir_path, csv_file['xi_filename'][idx]), dtype=\"<f4\").reshape(nt, Nx, Ny)\n",
    "    \n",
    "    uin = csv_file['u'][idx]\n",
    "    alpha = csv_file['alpha'][idx]\n",
    "\n",
    "    return theta, ustar, xi_f, uin, alpha, id\n",
    "\n",
    "# Function to extract fire positions\n",
    "def extract_fire_positions(xi_f):\n",
    "    return [np.argmax(np.mean(xi_f[t], axis=1)) for t in range(xi_f.shape[0])]\n",
    "\n",
    "# Prepare training data\n",
    "Datalist = []\n",
    "\n",
    "for idx in range(len(train_df)):\n",
    "    theta, ustar, xi_f, uin, alpha, id = load_dataX(idx, train_df, 'train')\n",
    "    \n",
    "    theta = torch.Tensor(theta).unsqueeze(1)\n",
    "    ustar = torch.Tensor(ustar).unsqueeze(1)\n",
    "    xi_f = torch.Tensor(xi_f).unsqueeze(1)\n",
    "    \n",
    "    uin_tensor = torch.zeros_like(xi_f) + uin\n",
    "    alpha_tensor = torch.zeros_like(xi_f) + alpha\n",
    "    \n",
    "    TUXUA = torch.cat([theta,ustar,xi_f, uin_tensor, alpha_tensor], dim=1)\n",
    "    TUXUA = TUXUA.unsqueeze(0)\n",
    "    \n",
    "    Datalist.append(TUXUA)\n",
    "    \n",
    "Data_train = torch.cat(Datalist)\n",
    "print(Data_train.shape)\n",
    "\n",
    "# Prepare testing data\n",
    "Datalist = []\n",
    "\n",
    "for idx in range(len(test_df)):\n",
    "    theta, ustar, xi_f, uin, alpha, id = load_dataX(idx, test_df, 'test')\n",
    "    \n",
    "    theta = torch.Tensor(theta).unsqueeze(1)\n",
    "    ustar = torch.Tensor(ustar).unsqueeze(1)\n",
    "    xi_f = torch.Tensor(xi_f).unsqueeze(1)\n",
    "    \n",
    "    uin_tensor = torch.zeros_like(xi_f) + uin\n",
    "    alpha_tensor = torch.zeros_like(xi_f) + alpha\n",
    "    \n",
    "    TUXUA = torch.cat([theta,ustar,xi_f, uin_tensor, alpha_tensor], dim=1)\n",
    "    TUXUA = TUXUA.unsqueeze(0)\n",
    "    \n",
    "    Datalist.append(TUXUA)\n",
    "    \n",
    "Data_test = torch.cat(Datalist)\n",
    "print(Data_test.shape)\n",
    "\n",
    "D1 = Data_train.reshape([Data_train.shape[0]*Data_train.shape[1], Data_train.shape[2], Data_train.shape[3], Data_train.shape[4]])\n",
    "D2 = Data_test.reshape([Data_test.shape[0]*Data_test.shape[1], Data_test.shape[2], Data_test.shape[3], Data_test.shape[4]])\n",
    "print(D1.shape, D2.shape) \n",
    "\n",
    "D = torch.cat([D1,D2], dim=0)\n",
    "MeanX = torch.mean(D, (0,2,3)).unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "StdX = torch.std(D, (0,2,3)).unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "print(StdX.shape)   # sample, timestep, channels [theta,ustar,xi_f, uin, alpha], X, Y\n",
    "print(MeanX.shape)\n",
    "\n",
    "MeanY = torch.mean(D, (0,2,3))[:3].unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "StdY = torch.std(D, (0,2,3))[:3].unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\n",
    "print(StdY.shape)   # sample, timestep, channels [theta,ustar,xi_f], X, Y\n",
    "print(MeanY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-17T21:54:31.096746Z",
     "iopub.status.busy": "2024-10-17T21:54:31.095854Z",
     "iopub.status.idle": "2024-10-17T21:54:31.106814Z",
     "shell.execute_reply": "2024-10-17T21:54:31.105528Z",
     "shell.execute_reply.started": "2024-10-17T21:54:31.096694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create custom PyTorch dataset\n",
    "class FlameDataset(Dataset):\n",
    "    def __init__(self, Data, history = 1, prediction = 1):\n",
    "        self.X = Data    #torch.Size([9, 150, 5, 113, 32])\n",
    "        self.history = history\n",
    "        self.prediction = prediction\n",
    "        self.count_cases = Data.shape[0]\n",
    "        self.count_timeIndices = Data.shape[1] - history - prediction + 1\n",
    "        self.indices = torch.arange(self.count_cases*self.count_timeIndices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        case = idx%self.count_cases\n",
    "        index = idx%self.count_timeIndices\n",
    "        \n",
    "        X = self.X[case,index:index + self.history,...]\n",
    "        Y = self.X[case,index + self.history:index + self.history + self.prediction,:3,...]\n",
    "        T = index + self.history #* torch.ones(idx.shape[0])\n",
    "        return X, Y, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T21:54:40.126578Z",
     "iopub.status.busy": "2024-10-17T21:54:40.126161Z",
     "iopub.status.idle": "2024-10-17T21:54:40.190155Z",
     "shell.execute_reply": "2024-10-17T21:54:40.189131Z",
     "shell.execute_reply.started": "2024-10-17T21:54:40.126533Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "\n",
    "        self.scale = (1 / (in_channels*out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes \n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-1)//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, num_channels, modes=16, width=64, initial_step=10):\n",
    "        super(FNO1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the initial condition and location (a(x), x)\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution of a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.padding = 2 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(initial_step*num_channels+1, self.width) # input channel is 2: (a(x), x)\n",
    "\n",
    "        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dim = [b, x1, t*v]\n",
    "        #x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = F.pad(x, [0, self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.unsqueeze(-2)\n",
    "\n",
    "\n",
    "class SpectralConv2d_fast(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d_fast, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, num_channels, modes1=12, modes2=12, width=20, initial_step=10, predict=1):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "        input shape: (batchsize, x, y, c)\n",
    "        output: the solution of the next timestep\n",
    "        output shape: (batchsize, x, y, c)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 2 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(initial_step*num_channels, self.width)\n",
    "        # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv3 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv2d(self.width, self.width, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, predict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dim = [b, x1, x2, t*v]   \n",
    "        #x = torch.cat((x, grid), dim=-1)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Pad tensor with boundary condition\n",
    "        x = F.pad(x, [0, self.padding, 0, self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding, :-self.padding] # Unpad the tensor\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SpectralConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):\n",
    "        super(SpectralConv3d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        3D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "        self.modes3 = modes3\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights3 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "        self.weights4 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul3d(self, input, weights):\n",
    "        # (batch, in_channel, x,y,t ), (in_channel, out_channel, x,y,t) -> (batch, out_channel, x,y,t)\n",
    "        return torch.einsum(\"bixyz,ioxyz->boxyz\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfftn(x, dim=[-3,-2,-1])\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-3), x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, :self.modes1, :self.modes2, :self.modes3], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, -self.modes1:, :self.modes2, :self.modes3], self.weights2)\n",
    "        out_ft[:, :, :self.modes1, -self.modes2:, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, :self.modes1, -self.modes2:, :self.modes3], self.weights3)\n",
    "        out_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3] = \\\n",
    "            self.compl_mul3d(x_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3], self.weights4)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfftn(out_ft, s=(x.size(-3), x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO3d(nn.Module):\n",
    "    def __init__(self, num_channels, modes1=8, modes2=8, modes3=8, width=20, initial_step=10):\n",
    "        super(FNO3d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t). It's a constant function in time, except for the last index.\n",
    "        input shape: (batchsize, x=64, y=64, t=40, c=13)\n",
    "        output: the solution of the next 40 timesteps\n",
    "        output shape: (batchsize, x=64, y=64, t=40, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.modes3 = modes3\n",
    "        self.width = width\n",
    "        self.padding = 6 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(initial_step*num_channels+3, self.width)\n",
    "        # input channel is 12: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t)\n",
    "\n",
    "        self.conv0 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv1 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv2 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.conv3 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n",
    "        self.w0 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.w3 = nn.Conv3d(self.width, self.width, 1)\n",
    "        self.bn0 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn1 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn2 = torch.nn.BatchNorm3d(self.width)\n",
    "        self.bn3 = torch.nn.BatchNorm3d(self.width)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 128)\n",
    "        self.fc2 = nn.Linear(128, num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dim = [b, x1, x2, x3, t*v]\n",
    "        #x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        x = F.pad(x, [0, self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv3(x)\n",
    "        x2 = self.w3(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x[..., :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 4, 1) # pad the domain if input is non-periodic\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.unsqueeze(-2)\n",
    "\n",
    "# model = FNO2d(num_channels=1, modes1=16, modes2=16, width=42, initial_step=history, predict=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-17T21:54:50.147630Z",
     "iopub.status.busy": "2024-10-17T21:54:50.147230Z",
     "iopub.status.idle": "2024-10-17T21:54:50.185431Z",
     "shell.execute_reply": "2024-10-17T21:54:50.184392Z",
     "shell.execute_reply.started": "2024-10-17T21:54:50.147591Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sinusoidal_embedding(n, d, device):\n",
    "    # Returns the standard positional embedding\n",
    "    embedding = torch.zeros(n, d).to(device)\n",
    "    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)]).to(device)\n",
    "    wk = wk.reshape((1, d))\n",
    "    t = torch.arange(n).reshape((n, 1)).to(device)\n",
    "    embedding[:, ::2] = torch.sin(t * wk[:, ::2]).to(device)\n",
    "    embedding[:, 1::2] = torch.cos(t * wk[:, ::2]).to(device)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "            #nn.LeakyReLU(1., inplace=True)    # to allow negative output\n",
    "        )\n",
    "        #self.fno_layer = FNO2d(num_channels=1, modes1=16, modes2=16, width=42, initial_step=out_channels, predict=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.double_conv(x)\n",
    "        #x = self.fno_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        #self.fno_layerUp = FNO2d(num_channels=1, modes1=16, modes2=16, width=42, initial_step=out_channels, predict=out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        #x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "        #                diffY // 2, diffY - diffY // 2])\n",
    "        #breakpoint()\n",
    "        # basically diffX diffY are placeholder without any value unless data is passed, so feature extractor fails\n",
    "        #x1 = F.pad(x1, (int(str(diffX)) // 2, int(str(diffX)) - int(str(diffX)) // 2,\n",
    "        #                int(str(diffY)) // 2, int(str(diffY)) - int(str(diffY)) // 2))     # get_graph_node_names was creating problem\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        #\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        #x = self.fno_layerUp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_filters = 4, n_channels=3, n_classes=1, n_inner=8, bilinear=False, uin_steps = 10, alpha_steps = 300, time_emb_dim=100, device = 'cpu'):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_inner = n_inner\n",
    "        self.bilinear = bilinear\n",
    "        self.device = device\n",
    "        self.uin_steps = uin_steps\n",
    "        self.alpha_steps = alpha_steps\n",
    "\n",
    "        # Sinusoidal embedding\n",
    "        self.alpha_embed = nn.Embedding(alpha_steps, time_emb_dim, device=device)\n",
    "        self.alpha_embed.weight.data = sinusoidal_embedding(alpha_steps, time_emb_dim, device)\n",
    "        self.alpha_embed.requires_grad_(False)\n",
    "        \n",
    "        self.uin_embed = nn.Embedding(uin_steps, time_emb_dim, device=device)\n",
    "        self.uin_embed.weight.data = sinusoidal_embedding(uin_steps, time_emb_dim, device)\n",
    "        self.uin_embed.requires_grad_(False)\n",
    "         \n",
    "        self.alpha_encoding_open = self._make_te(time_emb_dim, n_channels)\n",
    "        self.uin_encoding_open = self._make_te(time_emb_dim, n_channels)\n",
    "\n",
    "        self.alpha_encoding_inner = self._make_te(time_emb_dim, n_inner)\n",
    "        self.uin_encoding_inner = self._make_te(time_emb_dim, n_inner)\n",
    "\n",
    "        self.alpha_encoding_close = self._make_te(time_emb_dim, n_inner)\n",
    "        self.uin_encoding_close = self._make_te(time_emb_dim, n_inner)\n",
    "\n",
    "#         self.inc = (DoubleConv(n_channels, n_filters))\n",
    "        \n",
    "#         self.te1 = self._make_te(time_emb_dim, n_filters)\n",
    "#         self.down1 = (Down(n_filters, 2*n_filters))\n",
    "        \n",
    "#         self.te2 = self._make_te(time_emb_dim, 2*n_filters)\n",
    "#         self.down2 = (Down(2*n_filters, 4*n_filters))\n",
    "        \n",
    "#         self.te3 = self._make_te(time_emb_dim, 4*n_filters)\n",
    "#         self.down3 = (Down(4*n_filters, 8*n_filters))\n",
    "        \n",
    "#         factor = 2 if bilinear else 1\n",
    "        \n",
    "#         self.te4 = self._make_te(time_emb_dim, 8*n_filters)\n",
    "#         self.down4 = (Down(8*n_filters, 16*n_filters // factor))\n",
    "        \n",
    "#         self.teu1i = self._make_te(time_emb_dim, 16*n_filters)       \n",
    "#         self.teu1j = self._make_te(time_emb_dim, 8*n_filters)       \n",
    "#         self.up1 = (Up(16*n_filters, 8*n_filters // factor, bilinear))\n",
    "        \n",
    "#         self.teu2i = self._make_te(time_emb_dim, 8*n_filters)\n",
    "#         self.teu2j = self._make_te(time_emb_dim, 4*n_filters)       \n",
    "#         self.up2 = (Up(8*n_filters, 4*n_filters // factor, bilinear))\n",
    "        \n",
    "#         self.teu3i = self._make_te(time_emb_dim, 4*n_filters)\n",
    "#         self.teu3j = self._make_te(time_emb_dim, 2*n_filters)       \n",
    "#         self.up3 = (Up(4*n_filters, 2*n_filters // factor, bilinear))\n",
    "        \n",
    "#         self.teu4i = self._make_te(time_emb_dim, 2*n_filters)\n",
    "#         self.teu4j = self._make_te(time_emb_dim, n_filters)       \n",
    "#         self.up4 = (Up(2*n_filters, n_filters, bilinear))\n",
    "        \n",
    "#         self.teo = self._make_te(time_emb_dim, n_filters)\n",
    "#         self.outc = (OutConv(n_filters, n_classes)) # DoubleConv(n_fliters,n_classes)  # (OutConv(64, n_classes))    #1x1 conv can allow negative values in output to be mapped\n",
    "        self.fno_layer_1st = FNO2d(num_channels=1, modes1=16, modes2=16, width=96, initial_step=n_channels, predict=n_inner)\n",
    "        self.fno_layer_inner = FNO2d(num_channels=1, modes1=16, modes2=16, width=96, initial_step=n_inner, predict=n_inner)\n",
    "        self.fno_layer_last = FNO2d(num_channels=1, modes1=16, modes2=16, width=96, initial_step=n_inner, predict=n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        # x has 5 channels [theta,ustar,xi_f, uin_tensor, alpha_tensor]\n",
    "        # variables for embedding  # passed to nn positional encoding hence has to be integer or long integer\n",
    "        uin =x[:,0,3,0,0].long()   \n",
    "        alpha = (x[:,0,4,0,0]*10.).long()\n",
    "        \n",
    "        #print(alpha)\n",
    "        x = (x - MeanX)/StdX        \n",
    "        # print(x.shape)     # [32, 4, 5, 113, 32]\n",
    "        \n",
    "        x = x[:,:,:3,:,:]\n",
    "        Xshape = x.shape\n",
    "        #print(x.shape)\n",
    "        x = x.reshape([Xshape[0], Xshape[1] * Xshape[2], Xshape[3], Xshape[4]])\n",
    "        \n",
    "        n = len(x)\n",
    "        #x_ = x   \n",
    "        \n",
    "        uin = self.uin_embed(uin)  # takes in long or int tensor not float\n",
    "        alpha = self.alpha_embed(alpha)\n",
    "        \n",
    "        uin_open = self.uin_encoding_open(uin).reshape(n, -1, 1, 1)\n",
    "        alpha_open = self.uin_encoding_open(alpha).reshape(n, -1, 1, 1)\n",
    "        \n",
    "        uin_inner = self.uin_encoding_inner(uin).reshape(n, -1, 1, 1)\n",
    "        alpha_inner = self.uin_encoding_inner(alpha).reshape(n, -1, 1, 1)\n",
    "        \n",
    "        uin_close = self.uin_encoding_close(uin).reshape(n, -1, 1, 1)\n",
    "        alpha_close = self.uin_encoding_close(alpha).reshape(n, -1, 1, 1)\n",
    "        \n",
    "#         x1 = self.inc(x + self.tei(t).reshape(n, -1, 1, 1))\n",
    "#         x2 = self.down1(x1 + self.te1(t).reshape(n, -1, 1, 1))\n",
    "#         x3 = self.down2(x2 + self.te2(t).reshape(n, -1, 1, 1))\n",
    "#         x4 = self.down3(x3 + self.te3(t).reshape(n, -1, 1, 1))\n",
    "#         x5 = self.down4(x4 + self.te4(t).reshape(n, -1, 1, 1))\n",
    "#         x = self.up1(x5 + self.teu1i(t).reshape(n, -1, 1, 1), x4 + self.teu1j(t).reshape(n, -1, 1, 1))\n",
    "#         x = self.up2(x + self.teu2i(t).reshape(n, -1, 1, 1), x3 + self.teu2j(t).reshape(n, -1, 1, 1))\n",
    "#         x = self.up3(x + self.teu3i(t).reshape(n, -1, 1, 1), x2 + self.teu3j(t).reshape(n, -1, 1, 1))\n",
    "#         x = self.up4(x + self.teu4i(t).reshape(n, -1, 1, 1), x1 + self.teu4j(t).reshape(n, -1, 1, 1))\n",
    "#         x = self.outc(x + self.teo(t).reshape(n, -1, 1, 1))\n",
    "        \n",
    "        x = self.fno_layer_1st(x + uin_open + alpha_open)\n",
    "        for i in range(10):\n",
    "            x = self.fno_layer_inner(x + uin_inner + alpha_inner) + x\n",
    "        x = self.fno_layer_last(x + uin_close + alpha_close)\n",
    "        \n",
    "        x = x.reshape([Xshape[0], 1, 3, Xshape[3], Xshape[4]])\n",
    "        \n",
    "        x = (x * StdY) + MeanY\n",
    "        \n",
    "#         #x = 0.01*x + x_[:,-1:,:3,:,:]\n",
    "#         x[:,:,2:,:,:][x[:,:,2:,:,:]>=0.25] = 1.        \n",
    "        x[:,:,2:,:,:][x[:,:,2:,:,:]<0.02] = 0.        \n",
    "        \n",
    "        #print(x.shape)  32,T,3,113,32\n",
    "        \n",
    "        return x\n",
    "        # logits = self.outc(x)\n",
    "        # return logits\n",
    "        \n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out)\n",
    "        )\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-17T21:56:05.463437Z",
     "iopub.status.busy": "2024-10-17T21:56:05.463042Z",
     "iopub.status.idle": "2024-10-17T21:56:10.092761Z",
     "shell.execute_reply": "2024-10-17T21:56:10.091892Z",
     "shell.execute_reply.started": "2024-10-17T21:56:05.463397Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/12816653.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n",
      "/tmp/ipykernel_30/12816653.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters = 56783779\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "num_filters = 64\n",
    "history = 4\n",
    "prediction = 1\n",
    "\n",
    "# Create dataset and dataloaders for training and test sets\n",
    "train_dataset = FlameDataset(Data_train, history = history, prediction = prediction)\n",
    "test_dataset = FlameDataset(Data_test, history = history, prediction = prediction)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "#model = UNet(n_filters = num_filters, n_channels = 3 * history, n_classes = 3 * prediction, device=device).to(device)\n",
    "model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n",
    "#best_model = UNet(n_filters = num_filters, n_channels = 3 * history, n_classes = 3 * prediction, device=device).to(device)\n",
    "best_model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n",
    "print('Number of model parameters = %3d'%(count_parameters(model)))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_loss = []\n",
    "validation_loss = []\n",
    "best_loss = 10000\n",
    "best_epoch = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    for xx,yy,tt  in train_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xx, tt*0.)\n",
    "        \n",
    "        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n",
    "        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n",
    "        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n",
    "        \n",
    "        loss = theta_misfit + ustar_misfit + xi_f_misfit\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "#     for xx,yy,tt  in test_loader:\n",
    "#         xx = xx.to(device)\n",
    "#         yy = yy.to(device)\n",
    "#         tt = tt.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(xx, tt*0.)\n",
    "        \n",
    "#         theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n",
    "#         ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n",
    "#         xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n",
    "        \n",
    "#         loss = theta_misfit + ustar_misfit + xi_f_misfit\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "    for xx,yy,tt  in test_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(xx, tt*0.)\n",
    "        \n",
    "            val_loss = criterion(outputs, yy)\n",
    "        running_val_loss += val_loss.item()\n",
    "    \n",
    "    trn_loss = running_loss/len(train_loader)\n",
    "    val_loss = running_val_loss/len(test_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {trn_loss:.3e}, Validation Loss: {val_loss:.3e}\")\n",
    "    \n",
    "    train_loss.append(trn_loss)\n",
    "    validation_loss.append(val_loss)\n",
    "    \n",
    "    if val_loss <= best_loss:\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        torch.save(best_model, 'best_model.pth')\n",
    "        best_epoch = epoch\n",
    "        best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')\n",
    "print('best epoch: ', best_epoch + 1)\n",
    "\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model AGAIN\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 500\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    for xx,yy,tt  in train_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xx, tt*0.)\n",
    "        \n",
    "        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n",
    "        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n",
    "        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n",
    "        \n",
    "        loss = theta_misfit + ustar_misfit + xi_f_misfit\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    for xx,yy,tt  in test_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xx, tt*0.)\n",
    "        \n",
    "        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n",
    "        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n",
    "        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n",
    "        \n",
    "        loss = theta_misfit + ustar_misfit + xi_f_misfit\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    for xx,yy,tt  in test_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(xx, tt*0.)\n",
    "        \n",
    "            val_loss = criterion(outputs, yy)\n",
    "        running_val_loss += val_loss.item()\n",
    "    \n",
    "    trn_loss = running_loss/len(train_loader)\n",
    "    val_loss = running_val_loss/len(test_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {trn_loss:.3e}, Validation Loss: {val_loss:.3e}\")\n",
    "    \n",
    "    train_loss.append(trn_loss)\n",
    "    validation_loss.append(val_loss)\n",
    "    \n",
    "    if val_loss <= best_loss:\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        torch.save(best_model, 'best_model.pth')\n",
    "        best_epoch = epoch\n",
    "        best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T21:59:24.427894Z",
     "iopub.status.busy": "2024-10-17T21:59:24.427493Z",
     "iopub.status.idle": "2024-10-17T23:44:59.911769Z",
     "shell.execute_reply": "2024-10-17T23:44:59.910949Z",
     "shell.execute_reply.started": "2024-10-17T21:59:24.427860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 5.327e-03, Validation Loss: 6.947e+00\n",
      "Epoch [2/100], Train Loss: 5.513e-03, Validation Loss: 6.933e+00\n",
      "Epoch [3/100], Train Loss: 5.082e-03, Validation Loss: 6.922e+00\n",
      "Epoch [4/100], Train Loss: 5.073e-03, Validation Loss: 6.912e+00\n",
      "Epoch [5/100], Train Loss: 5.090e-03, Validation Loss: 6.898e+00\n",
      "Epoch [6/100], Train Loss: 5.095e-03, Validation Loss: 6.889e+00\n",
      "Epoch [7/100], Train Loss: 5.275e-03, Validation Loss: 6.876e+00\n",
      "Epoch [8/100], Train Loss: 5.277e-03, Validation Loss: 6.868e+00\n",
      "Epoch [9/100], Train Loss: 5.421e-03, Validation Loss: 6.857e+00\n",
      "Epoch [10/100], Train Loss: 5.064e-03, Validation Loss: 6.849e+00\n",
      "Epoch [11/100], Train Loss: 5.080e-03, Validation Loss: 6.839e+00\n",
      "Epoch [12/100], Train Loss: 5.084e-03, Validation Loss: 6.830e+00\n",
      "Epoch [13/100], Train Loss: 5.082e-03, Validation Loss: 6.819e+00\n",
      "Epoch [14/100], Train Loss: 5.065e-03, Validation Loss: 6.809e+00\n",
      "Epoch [15/100], Train Loss: 5.066e-03, Validation Loss: 6.801e+00\n",
      "Epoch [16/100], Train Loss: 5.070e-03, Validation Loss: 6.790e+00\n",
      "Epoch [17/100], Train Loss: 5.079e-03, Validation Loss: 6.782e+00\n",
      "Epoch [18/100], Train Loss: 5.069e-03, Validation Loss: 6.771e+00\n",
      "Epoch [19/100], Train Loss: 5.094e-03, Validation Loss: 6.768e+00\n",
      "Epoch [20/100], Train Loss: 5.511e-03, Validation Loss: 6.758e+00\n",
      "Epoch [21/100], Train Loss: 5.073e-03, Validation Loss: 6.745e+00\n",
      "Epoch [22/100], Train Loss: 5.092e-03, Validation Loss: 6.735e+00\n",
      "Epoch [23/100], Train Loss: 5.091e-03, Validation Loss: 6.734e+00\n",
      "Epoch [24/100], Train Loss: 5.063e-03, Validation Loss: 6.721e+00\n",
      "Epoch [25/100], Train Loss: 5.172e-03, Validation Loss: 6.712e+00\n",
      "Epoch [26/100], Train Loss: 5.220e-03, Validation Loss: 6.699e+00\n",
      "Epoch [27/100], Train Loss: 5.310e-03, Validation Loss: 6.691e+00\n",
      "Epoch [28/100], Train Loss: 5.067e-03, Validation Loss: 6.683e+00\n",
      "Epoch [29/100], Train Loss: 5.402e-03, Validation Loss: 6.674e+00\n",
      "Epoch [30/100], Train Loss: 5.362e-03, Validation Loss: 6.666e+00\n",
      "Epoch [31/100], Train Loss: 5.868e-03, Validation Loss: 6.658e+00\n",
      "Epoch [32/100], Train Loss: 5.064e-03, Validation Loss: 6.647e+00\n",
      "Epoch [33/100], Train Loss: 5.048e-03, Validation Loss: 6.640e+00\n",
      "Epoch [34/100], Train Loss: 5.066e-03, Validation Loss: 6.633e+00\n",
      "Epoch [35/100], Train Loss: 5.082e-03, Validation Loss: 6.624e+00\n",
      "Epoch [36/100], Train Loss: 5.065e-03, Validation Loss: 6.617e+00\n",
      "Epoch [37/100], Train Loss: 5.052e-03, Validation Loss: 6.608e+00\n",
      "Epoch [38/100], Train Loss: 5.328e-03, Validation Loss: 6.599e+00\n",
      "Epoch [39/100], Train Loss: 5.049e-03, Validation Loss: 6.590e+00\n",
      "Epoch [40/100], Train Loss: 5.071e-03, Validation Loss: 6.580e+00\n",
      "Epoch [41/100], Train Loss: 5.141e-03, Validation Loss: 6.574e+00\n",
      "Epoch [42/100], Train Loss: 5.051e-03, Validation Loss: 6.564e+00\n",
      "Epoch [43/100], Train Loss: 5.354e-03, Validation Loss: 6.558e+00\n",
      "Epoch [44/100], Train Loss: 5.060e-03, Validation Loss: 6.550e+00\n",
      "Epoch [45/100], Train Loss: 5.480e-03, Validation Loss: 6.545e+00\n",
      "Epoch [46/100], Train Loss: 5.054e-03, Validation Loss: 6.537e+00\n",
      "Epoch [47/100], Train Loss: 5.060e-03, Validation Loss: 6.536e+00\n",
      "Epoch [48/100], Train Loss: 5.273e-03, Validation Loss: 6.521e+00\n",
      "Epoch [49/100], Train Loss: 5.020e-03, Validation Loss: 6.514e+00\n",
      "Epoch [50/100], Train Loss: 5.029e-03, Validation Loss: 6.505e+00\n",
      "Epoch [51/100], Train Loss: 5.057e-03, Validation Loss: 6.497e+00\n",
      "Epoch [52/100], Train Loss: 5.008e-03, Validation Loss: 6.490e+00\n",
      "Epoch [53/100], Train Loss: 5.199e-03, Validation Loss: 6.483e+00\n",
      "Epoch [54/100], Train Loss: 5.511e-03, Validation Loss: 6.477e+00\n",
      "Epoch [55/100], Train Loss: 5.818e-03, Validation Loss: 6.470e+00\n",
      "Epoch [56/100], Train Loss: 5.066e-03, Validation Loss: 6.463e+00\n",
      "Epoch [57/100], Train Loss: 5.042e-03, Validation Loss: 6.456e+00\n",
      "Epoch [58/100], Train Loss: 5.300e-03, Validation Loss: 6.446e+00\n",
      "Epoch [59/100], Train Loss: 5.053e-03, Validation Loss: 6.437e+00\n",
      "Epoch [60/100], Train Loss: 5.018e-03, Validation Loss: 6.431e+00\n",
      "Epoch [61/100], Train Loss: 5.364e-03, Validation Loss: 6.423e+00\n",
      "Epoch [62/100], Train Loss: 5.039e-03, Validation Loss: 6.418e+00\n",
      "Epoch [63/100], Train Loss: 5.043e-03, Validation Loss: 6.410e+00\n",
      "Epoch [64/100], Train Loss: 5.049e-03, Validation Loss: 6.400e+00\n",
      "Epoch [65/100], Train Loss: 5.031e-03, Validation Loss: 6.400e+00\n",
      "Epoch [66/100], Train Loss: 5.058e-03, Validation Loss: 6.388e+00\n",
      "Epoch [67/100], Train Loss: 5.052e-03, Validation Loss: 6.382e+00\n",
      "Epoch [68/100], Train Loss: 5.038e-03, Validation Loss: 6.373e+00\n",
      "Epoch [69/100], Train Loss: 5.019e-03, Validation Loss: 6.370e+00\n",
      "Epoch [70/100], Train Loss: 5.031e-03, Validation Loss: 6.361e+00\n",
      "Epoch [71/100], Train Loss: 5.164e-03, Validation Loss: 6.354e+00\n",
      "Epoch [72/100], Train Loss: 5.036e-03, Validation Loss: 6.345e+00\n",
      "Epoch [73/100], Train Loss: 5.018e-03, Validation Loss: 6.340e+00\n",
      "Epoch [74/100], Train Loss: 5.057e-03, Validation Loss: 6.340e+00\n",
      "Epoch [75/100], Train Loss: 5.016e-03, Validation Loss: 6.331e+00\n",
      "Epoch [76/100], Train Loss: 5.873e-03, Validation Loss: 6.318e+00\n",
      "Epoch [77/100], Train Loss: 5.022e-03, Validation Loss: 6.311e+00\n",
      "Epoch [78/100], Train Loss: 5.148e-03, Validation Loss: 6.302e+00\n",
      "Epoch [79/100], Train Loss: 5.020e-03, Validation Loss: 6.296e+00\n",
      "Epoch [80/100], Train Loss: 5.146e-03, Validation Loss: 6.286e+00\n",
      "Epoch [81/100], Train Loss: 5.801e-03, Validation Loss: 6.280e+00\n",
      "Epoch [82/100], Train Loss: 5.136e-03, Validation Loss: 6.270e+00\n",
      "Epoch [83/100], Train Loss: 5.040e-03, Validation Loss: 6.265e+00\n",
      "Epoch [84/100], Train Loss: 5.021e-03, Validation Loss: 6.257e+00\n",
      "Epoch [85/100], Train Loss: 5.008e-03, Validation Loss: 6.254e+00\n",
      "Epoch [86/100], Train Loss: 5.017e-03, Validation Loss: 6.245e+00\n",
      "Epoch [87/100], Train Loss: 4.988e-03, Validation Loss: 6.242e+00\n",
      "Epoch [88/100], Train Loss: 5.145e-03, Validation Loss: 6.233e+00\n",
      "Epoch [89/100], Train Loss: 5.213e-03, Validation Loss: 6.231e+00\n",
      "Epoch [90/100], Train Loss: 5.041e-03, Validation Loss: 6.224e+00\n",
      "Epoch [91/100], Train Loss: 5.134e-03, Validation Loss: 6.221e+00\n",
      "Epoch [92/100], Train Loss: 4.990e-03, Validation Loss: 6.208e+00\n",
      "Epoch [93/100], Train Loss: 5.423e-03, Validation Loss: 6.201e+00\n",
      "Epoch [94/100], Train Loss: 5.041e-03, Validation Loss: 6.191e+00\n",
      "Epoch [95/100], Train Loss: 5.017e-03, Validation Loss: 6.183e+00\n",
      "Epoch [96/100], Train Loss: 4.994e-03, Validation Loss: 6.175e+00\n",
      "Epoch [97/100], Train Loss: 4.993e-03, Validation Loss: 6.169e+00\n",
      "Epoch [98/100], Train Loss: 5.182e-03, Validation Loss: 6.160e+00\n",
      "Epoch [99/100], Train Loss: 5.559e-03, Validation Loss: 6.158e+00\n",
      "Epoch [100/100], Train Loss: 5.218e-03, Validation Loss: 6.149e+00\n"
     ]
    }
   ],
   "source": [
    "# Train the model AGAIN\n",
    "#train_loss = []\n",
    "#validation_loss = []\n",
    "#best_loss = 10000\n",
    "#best_epoch = 0\n",
    "learning_rate = 1e-6\n",
    "num_epochs = 100\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    for xx,yy,tt  in train_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xx, tt*0.)\n",
    "        \n",
    "        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n",
    "        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n",
    "        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n",
    "        \n",
    "        loss = theta_misfit + ustar_misfit + xi_f_misfit\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    for xx,yy,tt  in test_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xx, tt*0.)\n",
    "        \n",
    "        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n",
    "        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n",
    "        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n",
    "        \n",
    "        loss = theta_misfit + ustar_misfit + xi_f_misfit\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    for xx,yy,tt  in test_loader:\n",
    "        xx = xx.to(device)\n",
    "        yy = yy.to(device)\n",
    "        tt = tt.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(xx, tt*0.)\n",
    "        \n",
    "            val_loss = criterion(outputs, yy)\n",
    "        running_val_loss += val_loss.item()\n",
    "    \n",
    "    trn_loss = running_loss/len(train_loader)\n",
    "    val_loss = running_val_loss/len(test_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {trn_loss:.3e}, Validation Loss: {val_loss:.3e}\")\n",
    "    \n",
    "    train_loss.append(trn_loss)\n",
    "    validation_loss.append(val_loss)\n",
    "    \n",
    "    if val_loss <= best_loss:\n",
    "        best_model.load_state_dict(model.state_dict())\n",
    "        torch.save(best_model, 'best_model.pth')\n",
    "        best_epoch = epoch\n",
    "        best_loss = val_loss\n",
    "    torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T23:50:54.929963Z",
     "iopub.status.busy": "2024-10-17T23:50:54.929091Z",
     "iopub.status.idle": "2024-10-17T23:50:56.413114Z",
     "shell.execute_reply": "2024-10-17T23:50:56.412082Z",
     "shell.execute_reply.started": "2024-10-17T23:50:54.929919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch:  100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model, 'model.pth')\n",
    "print('best epoch: ', best_epoch + 1)\n",
    "\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = best_model     # least validation does not work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the training and test sets\n",
    "running_test_loss = 0.0\n",
    "for xx,yy,tt  in test_loader:\n",
    "    xx = xx.to(device)\n",
    "    yy = yy.to(device)\n",
    "    tt = tt.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(xx, tt*0.)    \n",
    "        test_loss = criterion(outputs, yy)\n",
    "    running_test_loss += test_loss.item()\n",
    "    \n",
    "# Calculate the MSE test sets\n",
    "test_mse = running_test_loss/len(test_loader)\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "# Plot predictions for a few samples from the test set\n",
    "sample = 0\n",
    "for xx,yy,tt  in test_loader:\n",
    "    sample = sample + 1\n",
    "    xx = xx.to(device)\n",
    "    yy = yy.to(device)\n",
    "    tt = tt.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(xx, tt*0.)    \n",
    "    \n",
    "    fire_location_pred = outputs[0,0,2,:,:]\n",
    "    fire_location_pred = fire_location_pred.detach().cpu().numpy()\n",
    "    fire_location_true = yy[0,0,2,:,:]\n",
    "    fire_location_true = fire_location_true.detach().cpu().numpy()\n",
    "    print(\"sample: \", sample)\n",
    "    plt.imshow(fire_location_pred.T)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title(\"Fire Location Prediction\")\n",
    "    plt.show()\n",
    "    plt.imshow(fire_location_true.T)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title(\"Fire Location True\")\n",
    "    plt.show()\n",
    "    err = (fire_location_true.T - fire_location_pred.T)\n",
    "    plt.imshow(err)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title(\"Fire Location Error\")\n",
    "    plt.show()\n",
    "        \n",
    "    if sample >= 5:\n",
    "        break\n",
    "\n",
    "# # Prepare submission file\n",
    "# y_preds = {test_df['id'][idx]: pred_test_grid[idx][5:25].flatten(order='C').astype(np.float32) for idx in range(len(test_df))}\n",
    "\n",
    "# df = pd.DataFrame.from_dict(y_preds, orient='index')\n",
    "# df['id'] = df.index\n",
    "# df = df.reset_index(drop=True)\n",
    "# cols = ['id'] + df.columns.tolist()[:-1]\n",
    "# df = df[cols]\n",
    "\n",
    "# df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T23:51:13.441221Z",
     "iopub.status.busy": "2024-10-17T23:51:13.440854Z",
     "iopub.status.idle": "2024-10-17T23:51:50.117885Z",
     "shell.execute_reply": "2024-10-17T23:51:50.116928Z",
     "shell.execute_reply.started": "2024-10-17T23:51:13.441186Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27 entries, 0 to 26\n",
      "Columns: 72321 entries, id to 72319\n",
      "dtypes: float32(72320), int64(1)\n",
      "memory usage: 7.4 MB\n",
      "Generated Submission file\n"
     ]
    }
   ],
   "source": [
    "# Submission generation \n",
    "# 20 autoregression steps for 27 test samples\n",
    "y_preds = {}\n",
    "ids = []\n",
    "for idx in range(len(test_df)):\n",
    "    theta, ustar, xi_f, uin, alpha, id = load_dataX(idx, test_df, 'test')\n",
    "    \n",
    "    theta = torch.Tensor(theta).unsqueeze(1)\n",
    "    ustar = torch.Tensor(ustar).unsqueeze(1)\n",
    "    xi_f = torch.Tensor(xi_f).unsqueeze(1)\n",
    "    \n",
    "    uin_tensor = torch.zeros_like(xi_f) + uin\n",
    "    alpha_tensor = torch.zeros_like(xi_f) + alpha\n",
    "    \n",
    "    TUXUA = torch.cat([theta,ustar,xi_f, uin_tensor, alpha_tensor], dim=1)\n",
    "    TUXUA = TUXUA.unsqueeze(0)\n",
    "    \n",
    "    xx = TUXUA[:,-history:,...]\n",
    "    xx = xx.to(device)\n",
    "    \n",
    "    fire_loc = []\n",
    "    for i in range(20):\n",
    "        with torch.no_grad():        \n",
    "            output = model(xx, torch.zeros(1).to(device)) \n",
    "            Temp = torch.cat([output,xx[:,-1:,-2:,:,:]], dim=2) \n",
    "            xx = torch.cat([xx[:,1:,:,:,:], Temp], dim=1)\n",
    "            fire_loc.append(output[0,:,2,:,:])\n",
    "    \n",
    "    fire_location20 = torch.cat(fire_loc, dim=0)\n",
    "    fire_location20 = fire_location20.detach().cpu().numpy().flatten(order='C').astype(np.float32)\n",
    "    \n",
    "    y_preds[id]= fire_location20\n",
    "    ids.append(id)\n",
    "\n",
    "df = pd.DataFrame.from_dict(y_preds,orient='index')\n",
    "df['id'] = ids\n",
    "#df.info()\n",
    "\n",
    "#move id to first column\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n",
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#df.head()\n",
    "df.info()\n",
    "df.to_csv('submission.csv',index=False)\n",
    "print('Generated Submission file' )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9748215,
     "sourceId": 85210,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 140981,
     "modelInstanceId": 117746,
     "sourceId": 139056,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
