{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":85210,"databundleVersionId":9748215,"sourceType":"competition"},{"sourceId":139056,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":117746,"modelId":140981}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T21:54:09.046505Z","iopub.execute_input":"2024-10-17T21:54:09.047594Z","iopub.status.idle":"2024-10-17T21:54:09.053296Z","shell.execute_reply.started":"2024-10-17T21:54:09.047552Z","shell.execute_reply":"2024-10-17T21:54:09.052201Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    if torch.cuda.is_available():\n        device = 'cuda:0'\n    else:\n        device = 'cpu'\n    return device\n\ndevice = get_device()\nprint('Device: ',device)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T21:54:13.571919Z","iopub.execute_input":"2024-10-17T21:54:13.572623Z","iopub.status.idle":"2024-10-17T21:54:13.636311Z","shell.execute_reply.started":"2024-10-17T21:54:13.572583Z","shell.execute_reply":"2024-10-17T21:54:13.635348Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Device:  cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Paths\ninput_path = '/kaggle/input/2024-flame-ai-challenge/dataset/'\n\n# Load data\ntrain_df = pd.read_csv(os.path.join(input_path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(input_path, 'test.csv'))\n\n# Function to load data\ndef load_dataX(idx, df, data_dir):\n    csv_file = df.reset_index().to_dict(orient='list')\n    dir_path = os.path.join(input_path, data_dir)\n    \n    id = csv_file['id'][idx]\n    nt, Nx, Ny = csv_file['Nt'][idx], csv_file['Nx'][idx], csv_file['Ny'][idx]\n    \n    theta = np.fromfile(os.path.join(dir_path, csv_file['theta_filename'][idx]), dtype=\"<f4\").reshape(nt, Nx, Ny)\n    ustar = np.fromfile(os.path.join(dir_path, csv_file['ustar_filename'][idx]), dtype=\"<f4\").reshape(nt, Nx, Ny)\n    xi_f = np.fromfile(os.path.join(dir_path, csv_file['xi_filename'][idx]), dtype=\"<f4\").reshape(nt, Nx, Ny)\n    \n    uin = csv_file['u'][idx]\n    alpha = csv_file['alpha'][idx]\n\n    return theta, ustar, xi_f, uin, alpha, id\n\n# Function to extract fire positions\ndef extract_fire_positions(xi_f):\n    return [np.argmax(np.mean(xi_f[t], axis=1)) for t in range(xi_f.shape[0])]\n\n# Prepare training data\nDatalist = []\n\nfor idx in range(len(train_df)):\n    theta, ustar, xi_f, uin, alpha, id = load_dataX(idx, train_df, 'train')\n    \n    theta = torch.Tensor(theta).unsqueeze(1)\n    ustar = torch.Tensor(ustar).unsqueeze(1)\n    xi_f = torch.Tensor(xi_f).unsqueeze(1)\n    \n    uin_tensor = torch.zeros_like(xi_f) + uin\n    alpha_tensor = torch.zeros_like(xi_f) + alpha\n    \n    TUXUA = torch.cat([theta,ustar,xi_f, uin_tensor, alpha_tensor], dim=1)\n    TUXUA = TUXUA.unsqueeze(0)\n    \n    Datalist.append(TUXUA)\n    \nData_train = torch.cat(Datalist)\nprint(Data_train.shape)\n\n# Prepare testing data\nDatalist = []\n\nfor idx in range(len(test_df)):\n    theta, ustar, xi_f, uin, alpha, id = load_dataX(idx, test_df, 'test')\n    \n    theta = torch.Tensor(theta).unsqueeze(1)\n    ustar = torch.Tensor(ustar).unsqueeze(1)\n    xi_f = torch.Tensor(xi_f).unsqueeze(1)\n    \n    uin_tensor = torch.zeros_like(xi_f) + uin\n    alpha_tensor = torch.zeros_like(xi_f) + alpha\n    \n    TUXUA = torch.cat([theta,ustar,xi_f, uin_tensor, alpha_tensor], dim=1)\n    TUXUA = TUXUA.unsqueeze(0)\n    \n    Datalist.append(TUXUA)\n    \nData_test = torch.cat(Datalist)\nprint(Data_test.shape)\n\nD1 = Data_train.reshape([Data_train.shape[0]*Data_train.shape[1], Data_train.shape[2], Data_train.shape[3], Data_train.shape[4]])\nD2 = Data_test.reshape([Data_test.shape[0]*Data_test.shape[1], Data_test.shape[2], Data_test.shape[3], Data_test.shape[4]])\nprint(D1.shape, D2.shape) \n\nD = torch.cat([D1,D2], dim=0)\nMeanX = torch.mean(D, (0,2,3)).unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\nStdX = torch.std(D, (0,2,3)).unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\nprint(StdX.shape)   # sample, timestep, channels [theta,ustar,xi_f, uin, alpha], X, Y\nprint(MeanX.shape)\n\nMeanY = torch.mean(D, (0,2,3))[:3].unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\nStdY = torch.std(D, (0,2,3))[:3].unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1).to(device)\nprint(StdY.shape)   # sample, timestep, channels [theta,ustar,xi_f], X, Y\nprint(MeanY.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T21:54:21.067085Z","iopub.execute_input":"2024-10-17T21:54:21.067458Z","iopub.status.idle":"2024-10-17T21:54:22.964126Z","shell.execute_reply.started":"2024-10-17T21:54:21.067416Z","shell.execute_reply":"2024-10-17T21:54:22.963103Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"torch.Size([9, 150, 5, 113, 32])\ntorch.Size([27, 5, 5, 113, 32])\ntorch.Size([1350, 5, 113, 32]) torch.Size([135, 5, 113, 32])\ntorch.Size([1, 1, 5, 1, 1])\ntorch.Size([1, 1, 5, 1, 1])\ntorch.Size([1, 1, 3, 1, 1])\ntorch.Size([1, 1, 3, 1, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create custom PyTorch dataset\nclass FlameDataset(Dataset):\n    def __init__(self, Data, history = 1, prediction = 1):\n        self.X = Data    #torch.Size([9, 150, 5, 113, 32])\n        self.history = history\n        self.prediction = prediction\n        self.count_cases = Data.shape[0]\n        self.count_timeIndices = Data.shape[1] - history - prediction + 1\n        self.indices = torch.arange(self.count_cases*self.count_timeIndices)\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        case = idx%self.count_cases\n        index = idx%self.count_timeIndices\n        \n        X = self.X[case,index:index + self.history,...]\n        Y = self.X[case,index + self.history:index + self.history + self.prediction,:3,...]\n        T = index + self.history #* torch.ones(idx.shape[0])\n        return X, Y, T","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T21:54:31.095854Z","iopub.execute_input":"2024-10-17T21:54:31.096746Z","iopub.status.idle":"2024-10-17T21:54:31.106814Z","shell.execute_reply.started":"2024-10-17T21:54:31.096694Z","shell.execute_reply":"2024-10-17T21:54:31.105528Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class SpectralConv1d(nn.Module):\n    def __init__(self, in_channels, out_channels, modes1):\n        super(SpectralConv1d, self).__init__()\n\n        \"\"\"\n        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n        \"\"\"\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n\n        self.scale = (1 / (in_channels*out_channels))\n        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n\n    # Complex multiplication\n    def compl_mul1d(self, input, weights):\n        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n        return torch.einsum(\"bix,iox->box\", input, weights)\n\n    def forward(self, x):\n        batchsize = x.shape[0]\n        #Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfft(x)\n\n        # Multiply relevant Fourier modes \n        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-1)//2 + 1,  device=x.device, dtype=torch.cfloat)\n        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n\n        #Return to physical space\n        x = torch.fft.irfft(out_ft, n=x.size(-1))\n        return x\n\nclass FNO1d(nn.Module):\n    def __init__(self, num_channels, modes=16, width=64, initial_step=10):\n        super(FNO1d, self).__init__()\n\n        \"\"\"\n        The overall network. It contains 4 layers of the Fourier layer.\n        1. Lift the input to the desire channel dimension by self.fc0 .\n        2. 4 layers of the integral operators u' = (W + K)(u).\n            W defined by self.w; K defined by self.conv .\n        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n        \n        input: the solution of the initial condition and location (a(x), x)\n        input shape: (batchsize, x=s, c=2)\n        output: the solution of a later timestep\n        output shape: (batchsize, x=s, c=1)\n        \"\"\"\n\n        self.modes1 = modes\n        self.width = width\n        self.padding = 2 # pad the domain if input is non-periodic\n        self.fc0 = nn.Linear(initial_step*num_channels+1, self.width) # input channel is 2: (a(x), x)\n\n        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n        self.conv1 = SpectralConv1d(self.width, self.width, self.modes1)\n        self.conv2 = SpectralConv1d(self.width, self.width, self.modes1)\n        self.conv3 = SpectralConv1d(self.width, self.width, self.modes1)\n        self.w0 = nn.Conv1d(self.width, self.width, 1)\n        self.w1 = nn.Conv1d(self.width, self.width, 1)\n        self.w2 = nn.Conv1d(self.width, self.width, 1)\n        self.w3 = nn.Conv1d(self.width, self.width, 1)\n\n        self.fc1 = nn.Linear(self.width, 128)\n        self.fc2 = nn.Linear(128, num_channels)\n\n    def forward(self, x):\n        # x dim = [b, x1, t*v]\n        #x = torch.cat((x, grid), dim=-1)\n        x = self.fc0(x)\n        x = x.permute(0, 2, 1)\n        \n        x = F.pad(x, [0, self.padding]) # pad the domain if input is non-periodic\n\n        x1 = self.conv0(x)\n        x2 = self.w0(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv1(x)\n        x2 = self.w1(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv2(x)\n        x2 = self.w2(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv3(x)\n        x2 = self.w3(x)\n        x = x1 + x2\n\n        x = x[..., :-self.padding]\n        x = x.permute(0, 2, 1)\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.fc2(x)\n        return x.unsqueeze(-2)\n\n\nclass SpectralConv2d_fast(nn.Module):\n    def __init__(self, in_channels, out_channels, modes1, modes2):\n        super(SpectralConv2d_fast, self).__init__()\n\n        \"\"\"\n        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n        \"\"\"\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n        self.modes2 = modes2\n\n        self.scale = (1 / (in_channels * out_channels))\n        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n\n    # Complex multiplication\n    def compl_mul2d(self, input, weights):\n        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n\n    def forward(self, x):\n        batchsize = x.shape[0]\n        #Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfft2(x)\n\n        # Multiply relevant Fourier modes\n        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n        out_ft[:, :, :self.modes1, :self.modes2] = \\\n            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n\n        #Return to physical space\n        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n        return x\n\nclass FNO2d(nn.Module):\n    def __init__(self, num_channels, modes1=12, modes2=12, width=20, initial_step=10, predict=1):\n        super(FNO2d, self).__init__()\n\n        \"\"\"\n        The overall network. It contains 4 layers of the Fourier layer.\n        1. Lift the input to the desire channel dimension by self.fc0 .\n        2. 4 layers of the integral operators u' = (W + K)(u).\n            W defined by self.w; K defined by self.conv .\n        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n        \n        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n        input shape: (batchsize, x, y, c)\n        output: the solution of the next timestep\n        output shape: (batchsize, x, y, c)\n        \"\"\"\n\n        self.modes1 = modes1\n        self.modes2 = modes2\n        self.width = width\n        self.padding = 2 # pad the domain if input is non-periodic\n        self.fc0 = nn.Linear(initial_step*num_channels, self.width)\n        # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n\n        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n        self.conv2 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n        self.conv3 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n        self.w0 = nn.Conv2d(self.width, self.width, 1)\n        self.w1 = nn.Conv2d(self.width, self.width, 1)\n        self.w2 = nn.Conv2d(self.width, self.width, 1)\n        self.w3 = nn.Conv2d(self.width, self.width, 1)\n\n        self.fc1 = nn.Linear(self.width, 128)\n        self.fc2 = nn.Linear(128, predict)\n\n    def forward(self, x):\n        # x dim = [b, x1, x2, t*v]   \n        #x = torch.cat((x, grid), dim=-1)\n        x = x.permute(0, 2, 3, 1)\n        x = self.fc0(x)\n        x = x.permute(0, 3, 1, 2)\n        \n        # Pad tensor with boundary condition\n        x = F.pad(x, [0, self.padding, 0, self.padding])\n\n        x1 = self.conv0(x)\n        x2 = self.w0(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv1(x)\n        x2 = self.w1(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv2(x)\n        x2 = self.w2(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv3(x)\n        x2 = self.w3(x)\n        x = x1 + x2\n\n        x = x[..., :-self.padding, :-self.padding] # Unpad the tensor\n        x = x.permute(0, 2, 3, 1)\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.fc2(x)\n        x = x.permute(0, 3, 1, 2)\n        return x\n    \n\nclass SpectralConv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, modes1, modes2, modes3):\n        super(SpectralConv3d, self).__init__()\n\n        \"\"\"\n        3D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n        \"\"\"\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n        self.modes2 = modes2\n        self.modes3 = modes3\n\n        self.scale = (1 / (in_channels * out_channels))\n        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n        self.weights3 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n        self.weights4 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, self.modes3, dtype=torch.cfloat))\n\n    # Complex multiplication\n    def compl_mul3d(self, input, weights):\n        # (batch, in_channel, x,y,t ), (in_channel, out_channel, x,y,t) -> (batch, out_channel, x,y,t)\n        return torch.einsum(\"bixyz,ioxyz->boxyz\", input, weights)\n\n    def forward(self, x):\n        batchsize = x.shape[0]\n        #Compute Fourier coeffcients up to factor of e^(- something constant)\n        x_ft = torch.fft.rfftn(x, dim=[-3,-2,-1])\n\n        # Multiply relevant Fourier modes\n        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-3), x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n        out_ft[:, :, :self.modes1, :self.modes2, :self.modes3] = \\\n            self.compl_mul3d(x_ft[:, :, :self.modes1, :self.modes2, :self.modes3], self.weights1)\n        out_ft[:, :, -self.modes1:, :self.modes2, :self.modes3] = \\\n            self.compl_mul3d(x_ft[:, :, -self.modes1:, :self.modes2, :self.modes3], self.weights2)\n        out_ft[:, :, :self.modes1, -self.modes2:, :self.modes3] = \\\n            self.compl_mul3d(x_ft[:, :, :self.modes1, -self.modes2:, :self.modes3], self.weights3)\n        out_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3] = \\\n            self.compl_mul3d(x_ft[:, :, -self.modes1:, -self.modes2:, :self.modes3], self.weights4)\n\n        #Return to physical space\n        x = torch.fft.irfftn(out_ft, s=(x.size(-3), x.size(-2), x.size(-1)))\n        return x\n\nclass FNO3d(nn.Module):\n    def __init__(self, num_channels, modes1=8, modes2=8, modes3=8, width=20, initial_step=10):\n        super(FNO3d, self).__init__()\n\n        \"\"\"\n        The overall network. It contains 4 layers of the Fourier layer.\n        1. Lift the input to the desire channel dimension by self.fc0 .\n        2. 4 layers of the integral operators u' = (W + K)(u).\n            W defined by self.w; K defined by self.conv .\n        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n        \n        input: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t). It's a constant function in time, except for the last index.\n        input shape: (batchsize, x=64, y=64, t=40, c=13)\n        output: the solution of the next 40 timesteps\n        output shape: (batchsize, x=64, y=64, t=40, c=1)\n        \"\"\"\n\n        self.modes1 = modes1\n        self.modes2 = modes2\n        self.modes3 = modes3\n        self.width = width\n        self.padding = 6 # pad the domain if input is non-periodic\n        self.fc0 = nn.Linear(initial_step*num_channels+3, self.width)\n        # input channel is 12: the solution of the first 10 timesteps + 3 locations (u(1, x, y), ..., u(10, x, y),  x, y, t)\n\n        self.conv0 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n        self.conv1 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n        self.conv2 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n        self.conv3 = SpectralConv3d(self.width, self.width, self.modes1, self.modes2, self.modes3)\n        self.w0 = nn.Conv3d(self.width, self.width, 1)\n        self.w1 = nn.Conv3d(self.width, self.width, 1)\n        self.w2 = nn.Conv3d(self.width, self.width, 1)\n        self.w3 = nn.Conv3d(self.width, self.width, 1)\n        self.bn0 = torch.nn.BatchNorm3d(self.width)\n        self.bn1 = torch.nn.BatchNorm3d(self.width)\n        self.bn2 = torch.nn.BatchNorm3d(self.width)\n        self.bn3 = torch.nn.BatchNorm3d(self.width)\n\n        self.fc1 = nn.Linear(self.width, 128)\n        self.fc2 = nn.Linear(128, num_channels)\n\n    def forward(self, x):\n        # x dim = [b, x1, x2, x3, t*v]\n        #x = torch.cat((x, grid), dim=-1)\n        x = self.fc0(x)\n        x = x.permute(0, 4, 1, 2, 3)\n        \n        x = F.pad(x, [0, self.padding]) # pad the domain if input is non-periodic\n\n        x1 = self.conv0(x)\n        x2 = self.w0(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv1(x)\n        x2 = self.w1(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv2(x)\n        x2 = self.w2(x)\n        x = x1 + x2\n        x = F.gelu(x)\n\n        x1 = self.conv3(x)\n        x2 = self.w3(x)\n        x = x1 + x2\n\n        x = x[..., :-self.padding]\n        x = x.permute(0, 2, 3, 4, 1) # pad the domain if input is non-periodic\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.fc2(x)\n        return x.unsqueeze(-2)\n\n# model = FNO2d(num_channels=1, modes1=16, modes2=16, width=42, initial_step=history, predict=prediction)","metadata":{"execution":{"iopub.status.busy":"2024-10-17T21:54:40.126161Z","iopub.execute_input":"2024-10-17T21:54:40.126578Z","iopub.status.idle":"2024-10-17T21:54:40.190155Z","shell.execute_reply.started":"2024-10-17T21:54:40.126533Z","shell.execute_reply":"2024-10-17T21:54:40.189131Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef sinusoidal_embedding(n, d, device):\n    # Returns the standard positional embedding\n    embedding = torch.zeros(n, d).to(device)\n    wk = torch.tensor([1 / 10_000 ** (2 * j / d) for j in range(d)]).to(device)\n    wk = wk.reshape((1, d))\n    t = torch.arange(n).reshape((n, 1)).to(device)\n    embedding[:, ::2] = torch.sin(t * wk[:, ::2]).to(device)\n    embedding[:, 1::2] = torch.cos(t * wk[:, ::2]).to(device)\n\n    return embedding\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n            #nn.LeakyReLU(1., inplace=True)    # to allow negative output\n        )\n        #self.fno_layer = FNO2d(num_channels=1, modes1=16, modes2=16, width=42, initial_step=out_channels, predict=out_channels)\n\n    def forward(self, x):\n        x = self.double_conv(x)\n        #x = self.fno_layer(x)\n        return x\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n        #self.fno_layerUp = FNO2d(num_channels=1, modes1=16, modes2=16, width=42, initial_step=out_channels, predict=out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        #x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n        #                diffY // 2, diffY - diffY // 2])\n        #breakpoint()\n        # basically diffX diffY are placeholder without any value unless data is passed, so feature extractor fails\n        #x1 = F.pad(x1, (int(str(diffX)) // 2, int(str(diffX)) - int(str(diffX)) // 2,\n        #                int(str(diffY)) // 2, int(str(diffY)) - int(str(diffY)) // 2))     # get_graph_node_names was creating problem\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        #\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        #x = self.fno_layerUp(x)\n        return x\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_filters = 4, n_channels=3, n_classes=1, n_inner=8, bilinear=False, uin_steps = 10, alpha_steps = 300, time_emb_dim=100, device = 'cpu'):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels        \n        self.n_classes = n_classes\n        self.n_inner = n_inner\n        self.bilinear = bilinear\n        self.device = device\n        self.uin_steps = uin_steps\n        self.alpha_steps = alpha_steps\n\n        # Sinusoidal embedding\n        self.alpha_embed = nn.Embedding(alpha_steps, time_emb_dim, device=device)\n        self.alpha_embed.weight.data = sinusoidal_embedding(alpha_steps, time_emb_dim, device)\n        self.alpha_embed.requires_grad_(False)\n        \n        self.uin_embed = nn.Embedding(uin_steps, time_emb_dim, device=device)\n        self.uin_embed.weight.data = sinusoidal_embedding(uin_steps, time_emb_dim, device)\n        self.uin_embed.requires_grad_(False)\n         \n        self.alpha_encoding_open = self._make_te(time_emb_dim, n_channels)\n        self.uin_encoding_open = self._make_te(time_emb_dim, n_channels)\n\n        self.alpha_encoding_inner = self._make_te(time_emb_dim, n_inner)\n        self.uin_encoding_inner = self._make_te(time_emb_dim, n_inner)\n\n        self.alpha_encoding_close = self._make_te(time_emb_dim, n_inner)\n        self.uin_encoding_close = self._make_te(time_emb_dim, n_inner)\n\n#         self.inc = (DoubleConv(n_channels, n_filters))\n        \n#         self.te1 = self._make_te(time_emb_dim, n_filters)\n#         self.down1 = (Down(n_filters, 2*n_filters))\n        \n#         self.te2 = self._make_te(time_emb_dim, 2*n_filters)\n#         self.down2 = (Down(2*n_filters, 4*n_filters))\n        \n#         self.te3 = self._make_te(time_emb_dim, 4*n_filters)\n#         self.down3 = (Down(4*n_filters, 8*n_filters))\n        \n#         factor = 2 if bilinear else 1\n        \n#         self.te4 = self._make_te(time_emb_dim, 8*n_filters)\n#         self.down4 = (Down(8*n_filters, 16*n_filters // factor))\n        \n#         self.teu1i = self._make_te(time_emb_dim, 16*n_filters)       \n#         self.teu1j = self._make_te(time_emb_dim, 8*n_filters)       \n#         self.up1 = (Up(16*n_filters, 8*n_filters // factor, bilinear))\n        \n#         self.teu2i = self._make_te(time_emb_dim, 8*n_filters)\n#         self.teu2j = self._make_te(time_emb_dim, 4*n_filters)       \n#         self.up2 = (Up(8*n_filters, 4*n_filters // factor, bilinear))\n        \n#         self.teu3i = self._make_te(time_emb_dim, 4*n_filters)\n#         self.teu3j = self._make_te(time_emb_dim, 2*n_filters)       \n#         self.up3 = (Up(4*n_filters, 2*n_filters // factor, bilinear))\n        \n#         self.teu4i = self._make_te(time_emb_dim, 2*n_filters)\n#         self.teu4j = self._make_te(time_emb_dim, n_filters)       \n#         self.up4 = (Up(2*n_filters, n_filters, bilinear))\n        \n#         self.teo = self._make_te(time_emb_dim, n_filters)\n#         self.outc = (OutConv(n_filters, n_classes)) # DoubleConv(n_fliters,n_classes)  # (OutConv(64, n_classes))    #1x1 conv can allow negative values in output to be mapped\n        self.fno_layer_1st = FNO2d(num_channels=1, modes1=16, modes2=16, width=96, initial_step=n_channels, predict=n_inner)\n        self.fno_layer_inner = FNO2d(num_channels=1, modes1=16, modes2=16, width=96, initial_step=n_inner, predict=n_inner)\n        self.fno_layer_last = FNO2d(num_channels=1, modes1=16, modes2=16, width=96, initial_step=n_inner, predict=n_classes)\n        \n        \n    def forward(self, x, t):\n        \n        # x has 5 channels [theta,ustar,xi_f, uin_tensor, alpha_tensor]\n        # variables for embedding  # passed to nn positional encoding hence has to be integer or long integer\n        uin =x[:,0,3,0,0].long()   \n        alpha = (x[:,0,4,0,0]*10.).long()\n        \n        #print(alpha)\n        x = (x - MeanX)/StdX        \n        # print(x.shape)     # [32, 4, 5, 113, 32]\n        \n        x = x[:,:,:3,:,:]\n        Xshape = x.shape\n        #print(x.shape)\n        x = x.reshape([Xshape[0], Xshape[1] * Xshape[2], Xshape[3], Xshape[4]])\n        \n        n = len(x)\n        #x_ = x   \n        \n        uin = self.uin_embed(uin)  # takes in long or int tensor not float\n        alpha = self.alpha_embed(alpha)\n        \n        uin_open = self.uin_encoding_open(uin).reshape(n, -1, 1, 1)\n        alpha_open = self.uin_encoding_open(alpha).reshape(n, -1, 1, 1)\n        \n        uin_inner = self.uin_encoding_inner(uin).reshape(n, -1, 1, 1)\n        alpha_inner = self.uin_encoding_inner(alpha).reshape(n, -1, 1, 1)\n        \n        uin_close = self.uin_encoding_close(uin).reshape(n, -1, 1, 1)\n        alpha_close = self.uin_encoding_close(alpha).reshape(n, -1, 1, 1)\n        \n#         x1 = self.inc(x + self.tei(t).reshape(n, -1, 1, 1))\n#         x2 = self.down1(x1 + self.te1(t).reshape(n, -1, 1, 1))\n#         x3 = self.down2(x2 + self.te2(t).reshape(n, -1, 1, 1))\n#         x4 = self.down3(x3 + self.te3(t).reshape(n, -1, 1, 1))\n#         x5 = self.down4(x4 + self.te4(t).reshape(n, -1, 1, 1))\n#         x = self.up1(x5 + self.teu1i(t).reshape(n, -1, 1, 1), x4 + self.teu1j(t).reshape(n, -1, 1, 1))\n#         x = self.up2(x + self.teu2i(t).reshape(n, -1, 1, 1), x3 + self.teu2j(t).reshape(n, -1, 1, 1))\n#         x = self.up3(x + self.teu3i(t).reshape(n, -1, 1, 1), x2 + self.teu3j(t).reshape(n, -1, 1, 1))\n#         x = self.up4(x + self.teu4i(t).reshape(n, -1, 1, 1), x1 + self.teu4j(t).reshape(n, -1, 1, 1))\n#         x = self.outc(x + self.teo(t).reshape(n, -1, 1, 1))\n        \n        x = self.fno_layer_1st(x + uin_open + alpha_open)\n        for i in range(10):\n            x = self.fno_layer_inner(x + uin_inner + alpha_inner) + x\n        x = self.fno_layer_last(x + uin_close + alpha_close)\n        \n        x = x.reshape([Xshape[0], 1, 3, Xshape[3], Xshape[4]])\n        \n        x = (x * StdY) + MeanY\n        \n#         #x = 0.01*x + x_[:,-1:,:3,:,:]\n#         x[:,:,2:,:,:][x[:,:,2:,:,:]>=0.25] = 1.        \n        x[:,:,2:,:,:][x[:,:,2:,:,:]<0.02] = 0.        \n        \n        #print(x.shape)  32,T,3,113,32\n        \n        return x\n        # logits = self.outc(x)\n        # return logits\n        \n    def _make_te(self, dim_in, dim_out):\n        return nn.Sequential(\n            nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out)\n        )\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T21:54:50.147230Z","iopub.execute_input":"2024-10-17T21:54:50.147630Z","iopub.status.idle":"2024-10-17T21:54:50.185431Z","shell.execute_reply.started":"2024-10-17T21:54:50.147591Z","shell.execute_reply":"2024-10-17T21:54:50.184392Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Model parameters\nbatch_size = 32\nlearning_rate = 1e-3\nnum_epochs = 100\nnum_filters = 64\nhistory = 4\nprediction = 1\n\n# Create dataset and dataloaders for training and test sets\ntrain_dataset = FlameDataset(Data_train, history = history, prediction = prediction)\ntest_dataset = FlameDataset(Data_test, history = history, prediction = prediction)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Initialize the model, loss function, and optimizer\n#model = UNet(n_filters = num_filters, n_channels = 3 * history, n_classes = 3 * prediction, device=device).to(device)\nmodel = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n#best_model = UNet(n_filters = num_filters, n_channels = 3 * history, n_classes = 3 * prediction, device=device).to(device)\nbest_model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\nprint('Number of model parameters = %3d'%(count_parameters(model)))\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T21:56:05.463042Z","iopub.execute_input":"2024-10-17T21:56:05.463437Z","iopub.status.idle":"2024-10-17T21:56:10.092761Z","shell.execute_reply.started":"2024-10-17T21:56:05.463397Z","shell.execute_reply":"2024-10-17T21:56:10.091892Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/12816653.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n/tmp/ipykernel_30/12816653.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  best_model = torch.load('/kaggle/input/fno_resnet_vembedding/pytorch/default/1/best_model (9).pth')\n","output_type":"stream"},{"name":"stdout","text":"Number of model parameters = 56783779\n","output_type":"stream"}]},{"cell_type":"code","source":"#model = torch.load(\"model.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrain_loss = []\nvalidation_loss = []\nbest_loss = 10000\nbest_epoch = 0\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    running_val_loss = 0.0\n    for xx,yy,tt  in train_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        optimizer.zero_grad()\n        outputs = model(xx, tt*0.)\n        \n        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n        \n        loss = theta_misfit + ustar_misfit + xi_f_misfit\n        \n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n#     for xx,yy,tt  in test_loader:\n#         xx = xx.to(device)\n#         yy = yy.to(device)\n#         tt = tt.to(device)\n#         optimizer.zero_grad()\n#         outputs = model(xx, tt*0.)\n        \n#         theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n#         ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n#         xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n        \n#         loss = theta_misfit + ustar_misfit + xi_f_misfit\n        \n#         loss.backward()\n#         optimizer.step()\n    \n    for xx,yy,tt  in test_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        with torch.no_grad():\n            outputs = model(xx, tt*0.)\n        \n            val_loss = criterion(outputs, yy)\n        running_val_loss += val_loss.item()\n    \n    trn_loss = running_loss/len(train_loader)\n    val_loss = running_val_loss/len(test_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {trn_loss:.3e}, Validation Loss: {val_loss:.3e}\")\n    \n    train_loss.append(trn_loss)\n    validation_loss.append(val_loss)\n    \n    if val_loss <= best_loss:\n        best_model.load_state_dict(model.state_dict())\n        torch.save(best_model, 'best_model.pth')\n        best_epoch = epoch\n        best_loss = val_loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'model.pth')\nprint('best epoch: ', best_epoch + 1)\n\nplt.plot(train_loss, label=\"Train Loss\")\nplt.plot(validation_loss, label=\"Validation Loss\")\nplt.legend()\nplt.yscale(\"log\")\nplt.title(\"Loss Curve\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model AGAIN\nlearning_rate = 1e-4\nnum_epochs = 500\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    running_val_loss = 0.0\n    for xx,yy,tt  in train_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        optimizer.zero_grad()\n        outputs = model(xx, tt*0.)\n        \n        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n        \n        loss = theta_misfit + ustar_misfit + xi_f_misfit\n        \n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    for xx,yy,tt  in test_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        optimizer.zero_grad()\n        outputs = model(xx, tt*0.)\n        \n        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n        \n        loss = theta_misfit + ustar_misfit + xi_f_misfit\n        \n        loss.backward()\n        optimizer.step()\n    \n    for xx,yy,tt  in test_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        with torch.no_grad():\n            outputs = model(xx, tt*0.)\n        \n            val_loss = criterion(outputs, yy)\n        running_val_loss += val_loss.item()\n    \n    trn_loss = running_loss/len(train_loader)\n    val_loss = running_val_loss/len(test_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {trn_loss:.3e}, Validation Loss: {val_loss:.3e}\")\n    \n    train_loss.append(trn_loss)\n    validation_loss.append(val_loss)\n    \n    if val_loss <= best_loss:\n        best_model.load_state_dict(model.state_dict())\n        torch.save(best_model, 'best_model.pth')\n        best_epoch = epoch\n        best_loss = val_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model AGAIN\n#train_loss = []\n#validation_loss = []\n#best_loss = 10000\n#best_epoch = 0\nlearning_rate = 1e-6\nnum_epochs = 100\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    running_val_loss = 0.0\n    for xx,yy,tt  in train_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        optimizer.zero_grad()\n        outputs = model(xx, tt*0.)\n        \n        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n        \n        loss = theta_misfit + ustar_misfit + xi_f_misfit\n        \n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    \n    for xx,yy,tt  in test_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        optimizer.zero_grad()\n        outputs = model(xx, tt*0.)\n        \n        theta_misfit = criterion(outputs[:,:,0:1,:,:], yy[:,:,0:1,:,:])/criterion(outputs[:,:,0:1,:,:]*0., yy[:,:,0:1,:,:])  \n        ustar_misfit = criterion(outputs[:,:,1:2,:,:], yy[:,:,1:2,:,:])/criterion(outputs[:,:,1:2,:,:]*0., yy[:,:,1:2,:,:]) \n        xi_f_misfit = criterion(outputs[:,:,2:3,:,:], yy[:,:,2:3,:,:])/criterion(outputs[:,:,2:3,:,:]*0., yy[:,:,2:3,:,:])\n        \n        loss = theta_misfit + ustar_misfit + xi_f_misfit\n        \n        loss.backward()\n        optimizer.step()\n    \n    for xx,yy,tt  in test_loader:\n        xx = xx.to(device)\n        yy = yy.to(device)\n        tt = tt.to(device)\n        with torch.no_grad():\n            outputs = model(xx, tt*0.)\n        \n            val_loss = criterion(outputs, yy)\n        running_val_loss += val_loss.item()\n    \n    trn_loss = running_loss/len(train_loader)\n    val_loss = running_val_loss/len(test_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {trn_loss:.3e}, Validation Loss: {val_loss:.3e}\")\n    \n    train_loss.append(trn_loss)\n    validation_loss.append(val_loss)\n    \n    if val_loss <= best_loss:\n        best_model.load_state_dict(model.state_dict())\n        torch.save(best_model, 'best_model.pth')\n        best_epoch = epoch\n        best_loss = val_loss\n    torch.save(model, 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T21:59:24.427493Z","iopub.execute_input":"2024-10-17T21:59:24.427894Z","iopub.status.idle":"2024-10-17T23:44:59.911769Z","shell.execute_reply.started":"2024-10-17T21:59:24.427860Z","shell.execute_reply":"2024-10-17T23:44:59.910949Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch [1/100], Train Loss: 5.327e-03, Validation Loss: 6.947e+00\nEpoch [2/100], Train Loss: 5.513e-03, Validation Loss: 6.933e+00\nEpoch [3/100], Train Loss: 5.082e-03, Validation Loss: 6.922e+00\nEpoch [4/100], Train Loss: 5.073e-03, Validation Loss: 6.912e+00\nEpoch [5/100], Train Loss: 5.090e-03, Validation Loss: 6.898e+00\nEpoch [6/100], Train Loss: 5.095e-03, Validation Loss: 6.889e+00\nEpoch [7/100], Train Loss: 5.275e-03, Validation Loss: 6.876e+00\nEpoch [8/100], Train Loss: 5.277e-03, Validation Loss: 6.868e+00\nEpoch [9/100], Train Loss: 5.421e-03, Validation Loss: 6.857e+00\nEpoch [10/100], Train Loss: 5.064e-03, Validation Loss: 6.849e+00\nEpoch [11/100], Train Loss: 5.080e-03, Validation Loss: 6.839e+00\nEpoch [12/100], Train Loss: 5.084e-03, Validation Loss: 6.830e+00\nEpoch [13/100], Train Loss: 5.082e-03, Validation Loss: 6.819e+00\nEpoch [14/100], Train Loss: 5.065e-03, Validation Loss: 6.809e+00\nEpoch [15/100], Train Loss: 5.066e-03, Validation Loss: 6.801e+00\nEpoch [16/100], Train Loss: 5.070e-03, Validation Loss: 6.790e+00\nEpoch [17/100], Train Loss: 5.079e-03, Validation Loss: 6.782e+00\nEpoch [18/100], Train Loss: 5.069e-03, Validation Loss: 6.771e+00\nEpoch [19/100], Train Loss: 5.094e-03, Validation Loss: 6.768e+00\nEpoch [20/100], Train Loss: 5.511e-03, Validation Loss: 6.758e+00\nEpoch [21/100], Train Loss: 5.073e-03, Validation Loss: 6.745e+00\nEpoch [22/100], Train Loss: 5.092e-03, Validation Loss: 6.735e+00\nEpoch [23/100], Train Loss: 5.091e-03, Validation Loss: 6.734e+00\nEpoch [24/100], Train Loss: 5.063e-03, Validation Loss: 6.721e+00\nEpoch [25/100], Train Loss: 5.172e-03, Validation Loss: 6.712e+00\nEpoch [26/100], Train Loss: 5.220e-03, Validation Loss: 6.699e+00\nEpoch [27/100], Train Loss: 5.310e-03, Validation Loss: 6.691e+00\nEpoch [28/100], Train Loss: 5.067e-03, Validation Loss: 6.683e+00\nEpoch [29/100], Train Loss: 5.402e-03, Validation Loss: 6.674e+00\nEpoch [30/100], Train Loss: 5.362e-03, Validation Loss: 6.666e+00\nEpoch [31/100], Train Loss: 5.868e-03, Validation Loss: 6.658e+00\nEpoch [32/100], Train Loss: 5.064e-03, Validation Loss: 6.647e+00\nEpoch [33/100], Train Loss: 5.048e-03, Validation Loss: 6.640e+00\nEpoch [34/100], Train Loss: 5.066e-03, Validation Loss: 6.633e+00\nEpoch [35/100], Train Loss: 5.082e-03, Validation Loss: 6.624e+00\nEpoch [36/100], Train Loss: 5.065e-03, Validation Loss: 6.617e+00\nEpoch [37/100], Train Loss: 5.052e-03, Validation Loss: 6.608e+00\nEpoch [38/100], Train Loss: 5.328e-03, Validation Loss: 6.599e+00\nEpoch [39/100], Train Loss: 5.049e-03, Validation Loss: 6.590e+00\nEpoch [40/100], Train Loss: 5.071e-03, Validation Loss: 6.580e+00\nEpoch [41/100], Train Loss: 5.141e-03, Validation Loss: 6.574e+00\nEpoch [42/100], Train Loss: 5.051e-03, Validation Loss: 6.564e+00\nEpoch [43/100], Train Loss: 5.354e-03, Validation Loss: 6.558e+00\nEpoch [44/100], Train Loss: 5.060e-03, Validation Loss: 6.550e+00\nEpoch [45/100], Train Loss: 5.480e-03, Validation Loss: 6.545e+00\nEpoch [46/100], Train Loss: 5.054e-03, Validation Loss: 6.537e+00\nEpoch [47/100], Train Loss: 5.060e-03, Validation Loss: 6.536e+00\nEpoch [48/100], Train Loss: 5.273e-03, Validation Loss: 6.521e+00\nEpoch [49/100], Train Loss: 5.020e-03, Validation Loss: 6.514e+00\nEpoch [50/100], Train Loss: 5.029e-03, Validation Loss: 6.505e+00\nEpoch [51/100], Train Loss: 5.057e-03, Validation Loss: 6.497e+00\nEpoch [52/100], Train Loss: 5.008e-03, Validation Loss: 6.490e+00\nEpoch [53/100], Train Loss: 5.199e-03, Validation Loss: 6.483e+00\nEpoch [54/100], Train Loss: 5.511e-03, Validation Loss: 6.477e+00\nEpoch [55/100], Train Loss: 5.818e-03, Validation Loss: 6.470e+00\nEpoch [56/100], Train Loss: 5.066e-03, Validation Loss: 6.463e+00\nEpoch [57/100], Train Loss: 5.042e-03, Validation Loss: 6.456e+00\nEpoch [58/100], Train Loss: 5.300e-03, Validation Loss: 6.446e+00\nEpoch [59/100], Train Loss: 5.053e-03, Validation Loss: 6.437e+00\nEpoch [60/100], Train Loss: 5.018e-03, Validation Loss: 6.431e+00\nEpoch [61/100], Train Loss: 5.364e-03, Validation Loss: 6.423e+00\nEpoch [62/100], Train Loss: 5.039e-03, Validation Loss: 6.418e+00\nEpoch [63/100], Train Loss: 5.043e-03, Validation Loss: 6.410e+00\nEpoch [64/100], Train Loss: 5.049e-03, Validation Loss: 6.400e+00\nEpoch [65/100], Train Loss: 5.031e-03, Validation Loss: 6.400e+00\nEpoch [66/100], Train Loss: 5.058e-03, Validation Loss: 6.388e+00\nEpoch [67/100], Train Loss: 5.052e-03, Validation Loss: 6.382e+00\nEpoch [68/100], Train Loss: 5.038e-03, Validation Loss: 6.373e+00\nEpoch [69/100], Train Loss: 5.019e-03, Validation Loss: 6.370e+00\nEpoch [70/100], Train Loss: 5.031e-03, Validation Loss: 6.361e+00\nEpoch [71/100], Train Loss: 5.164e-03, Validation Loss: 6.354e+00\nEpoch [72/100], Train Loss: 5.036e-03, Validation Loss: 6.345e+00\nEpoch [73/100], Train Loss: 5.018e-03, Validation Loss: 6.340e+00\nEpoch [74/100], Train Loss: 5.057e-03, Validation Loss: 6.340e+00\nEpoch [75/100], Train Loss: 5.016e-03, Validation Loss: 6.331e+00\nEpoch [76/100], Train Loss: 5.873e-03, Validation Loss: 6.318e+00\nEpoch [77/100], Train Loss: 5.022e-03, Validation Loss: 6.311e+00\nEpoch [78/100], Train Loss: 5.148e-03, Validation Loss: 6.302e+00\nEpoch [79/100], Train Loss: 5.020e-03, Validation Loss: 6.296e+00\nEpoch [80/100], Train Loss: 5.146e-03, Validation Loss: 6.286e+00\nEpoch [81/100], Train Loss: 5.801e-03, Validation Loss: 6.280e+00\nEpoch [82/100], Train Loss: 5.136e-03, Validation Loss: 6.270e+00\nEpoch [83/100], Train Loss: 5.040e-03, Validation Loss: 6.265e+00\nEpoch [84/100], Train Loss: 5.021e-03, Validation Loss: 6.257e+00\nEpoch [85/100], Train Loss: 5.008e-03, Validation Loss: 6.254e+00\nEpoch [86/100], Train Loss: 5.017e-03, Validation Loss: 6.245e+00\nEpoch [87/100], Train Loss: 4.988e-03, Validation Loss: 6.242e+00\nEpoch [88/100], Train Loss: 5.145e-03, Validation Loss: 6.233e+00\nEpoch [89/100], Train Loss: 5.213e-03, Validation Loss: 6.231e+00\nEpoch [90/100], Train Loss: 5.041e-03, Validation Loss: 6.224e+00\nEpoch [91/100], Train Loss: 5.134e-03, Validation Loss: 6.221e+00\nEpoch [92/100], Train Loss: 4.990e-03, Validation Loss: 6.208e+00\nEpoch [93/100], Train Loss: 5.423e-03, Validation Loss: 6.201e+00\nEpoch [94/100], Train Loss: 5.041e-03, Validation Loss: 6.191e+00\nEpoch [95/100], Train Loss: 5.017e-03, Validation Loss: 6.183e+00\nEpoch [96/100], Train Loss: 4.994e-03, Validation Loss: 6.175e+00\nEpoch [97/100], Train Loss: 4.993e-03, Validation Loss: 6.169e+00\nEpoch [98/100], Train Loss: 5.182e-03, Validation Loss: 6.160e+00\nEpoch [99/100], Train Loss: 5.559e-03, Validation Loss: 6.158e+00\nEpoch [100/100], Train Loss: 5.218e-03, Validation Loss: 6.149e+00\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, 'model.pth')\nprint('best epoch: ', best_epoch + 1)\n\nplt.plot(train_loss, label=\"Train Loss\")\nplt.plot(validation_loss, label=\"Validation Loss\")\nplt.legend()\nplt.yscale(\"log\")\nplt.title(\"Loss Curve\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T23:50:54.929091Z","iopub.execute_input":"2024-10-17T23:50:54.929963Z","iopub.status.idle":"2024-10-17T23:50:56.413114Z","shell.execute_reply.started":"2024-10-17T23:50:54.929919Z","shell.execute_reply":"2024-10-17T23:50:56.412082Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"best epoch:  100\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAi4AAAGzCAYAAAAIWpzfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE/ElEQVR4nO3deXgUVf7+/bu6OwvZIUAW9iUIQQSEgCCjIFFABhFRkUEE3EaFUWRm3BgccB+3H6NEne/4CDrjghvquIIIgoKsBkE2QYQIJGxmJ1v3ef4IadJJgACBppr367r6SrqquupTJ93pu8+pqraMMUYAAAA24PB3AQAAALVFcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAEAALZBcAHOYrNmzZJlWVq5cqW/S6mV9PR0XX/99WrWrJlCQkLUoEEDpaamaubMmXK73f4uD8Bp4PJ3AQBQGy+//LJuu+02xcXFafTo0UpKSlJeXp7mz5+vm266Sbt379YDDzzg7zIBnGIEFwBnvO+++0633XabevXqpU8//VSRkZHeeRMnTtTKlSu1bt26OtlWQUGBwsPD62RdAOoeQ0UAjun777/XoEGDFBUVpYiICPXv31/fffedzzKlpaWaNm2akpKSFBoaqtjYWPXp00fz5s3zLpOZmalx48apadOmCgkJUUJCgoYOHapffvnlqNufNm2aLMvS66+/7hNaKnTv3l1jx46VJC1cuFCWZWnhwoU+y/zyyy+yLEuzZs3yThs7dqwiIiK0detWXX755YqMjNSoUaM0YcIERUREqLCwsNq2Ro4cqfj4eJ+hqc8++0y/+93vFB4ersjISA0ePFg//vjjUfcJwIkhuAA4qh9//FG/+93vtGbNGt1zzz2aMmWKtm3bpr59+2rZsmXe5aZOnapp06apX79+mjFjhiZPnqzmzZtr9erV3mWGDx+uOXPmaNy4cXrhhRd05513Ki8vTzt27Dji9gsLCzV//nxddNFFat68eZ3vX1lZmQYMGKDGjRvr6aef1vDhwzVixAgVFBTok08+qVbL//73P1199dVyOp2SpP/85z8aPHiwIiIi9I9//ENTpkzR+vXr1adPn2MGMgAnwAA4a82cOdNIMitWrDjiMldeeaUJDg42W7du9U7btWuXiYyMNBdddJF3WufOnc3gwYOPuJ7ffvvNSDJPPfXUcdW4Zs0aI8ncddddtVp+wYIFRpJZsGCBz/Rt27YZSWbmzJneaWPGjDGSzH333eezrMfjMU2aNDHDhw/3mf72228bSWbRokXGGGPy8vJMTEyMueWWW3yWy8zMNNHR0dWmAzh59LgAOCK32625c+fqyiuvVOvWrb3TExIS9Ic//EHffPONcnNzJUkxMTH68ccf9dNPP9W4rnr16ik4OFgLFy7Ub7/9VusaKtZf0xBRXbn99tt97luWpWuuuUaffvqp8vPzvdNnz56tJk2aqE+fPpKkefPmKTs7WyNHjtS+ffu8N6fTqZ49e2rBggWnrGbgbEVwAXBEe/fuVWFhoc4555xq8zp06CCPx6OMjAxJ0kMPPaTs7Gy1a9dOnTp10l//+lf98MMP3uVDQkL0j3/8Q5999pni4uJ00UUX6cknn1RmZuZRa4iKipIk5eXl1eGeHeZyudS0adNq00eMGKGDBw/qo48+kiTl5+fr008/1TXXXCPLsiTJG9IuueQSNWrUyOc2d+5c7dmz55TUDJzNCC4A6sRFF12krVu36pVXXtG5556rl19+Weeff75efvll7zITJ07U5s2b9fjjjys0NFRTpkxRhw4d9P333x9xvW3btpXL5dLatWtrVUdFqKjqSNd5CQkJkcNR/V/hBRdcoJYtW+rtt9+WJP3vf//TwYMHNWLECO8yHo9HUvlxLvPmzat2+/DDD2tVM4DaI7gAOKJGjRopLCxMmzZtqjZv48aNcjgcatasmXdagwYNNG7cOL355pvKyMjQeeedp6lTp/o8rk2bNvrzn/+suXPnat26dSopKdEzzzxzxBrCwsJ0ySWXaNGiRd7enaOpX7++JCk7O9tn+vbt24/52KquvfZaff7558rNzdXs2bPVsmVLXXDBBT77IkmNGzdWampqtVvfvn2Pe5sAjo7gAuCInE6nLrvsMn344Yc+Z8hkZWXpjTfeUJ8+fbxDOfv37/d5bEREhNq2bavi4mJJ5WfkFBUV+SzTpk0bRUZGepc5kr///e8yxmj06NE+x5xUWLVqlV599VVJUosWLeR0OrVo0SKfZV544YXa7XQlI0aMUHFxsV599VV9/vnnuvbaa33mDxgwQFFRUXrsscdUWlpa7fF79+497m0CODouQAdAr7zyij7//PNq0++66y498sgjmjdvnvr06aM77rhDLpdL//rXv1RcXKwnn3zSu2xycrL69u2rbt26qUGDBlq5cqXeffddTZgwQZK0efNm9e/fX9dee62Sk5Plcrk0Z84cZWVl6brrrjtqfb1791ZaWpruuOMOtW/f3ufKuQsXLtRHH32kRx55RJIUHR2ta665Rs8//7wsy1KbNm308ccfn9DxJueff77atm2ryZMnq7i42GeYSCo//ubFF1/U6NGjdf755+u6665To0aNtGPHDn3yySe68MILNWPGjOPeLoCj8PdpTQD8p+J06CPdMjIyjDHGrF692gwYMMBERESYsLAw069fP7NkyRKfdT3yyCOmR48eJiYmxtSrV8+0b9/ePProo6akpMQYY8y+ffvM+PHjTfv27U14eLiJjo42PXv2NG+//Xat6121apX5wx/+YBITE01QUJCpX7++6d+/v3n11VeN2+32Lrd3714zfPhwExYWZurXr2/++Mc/mnXr1tV4OnR4ePhRtzl58mQjybRt2/aIyyxYsMAMGDDAREdHm9DQUNOmTRszduxYs3LlylrvG4DasYwxxm+pCQAA4DhwjAsAALANggsAALANggsAALANggsAALANggsAALANggsAALCNgLsAncfj0a5duxQZGXnE7ywBAABnFmOM8vLylJiYWOP3h1UIuOCya9cun+9OAQAA9pGRkVHjN7ZXCLjgEhkZKal8xyu+QwUAAJzZcnNz1axZM+/7+JEEXHCpGB6KiooiuAAAYDPHOsyDg3MBAIBtBExwSUtLU3JyslJSUvxdCgAAOEUC7ksWc3NzFR0drZycHIaKAACwidq+fwdMjwsAAAh8BBcAAGAbBBcAAGAbBBcAAGAbARNcOKsIAIDAx1lFAADA7zirCAAABByCCwAAsA2CCwAAsI2A+5LFU2bd+1LGMsnhkpxBkiPo0E/X4Z8Ol+RwHv7dch6+732Mq/xnTcs7XIfnO4MqPd556PdKjznGl1ABABCIAia4pKWlKS0tTW63+9RsYNsiadXMU7PuE+ENMhWhxuE7zWee63AA8gaqykHJVT1IeZdx+AawI4Uzy1E9hHnX5Si/yTr8u8NRZR1VQpujhrosR6XAduhn5eBYEQ4JdwAQsDirqLY2fiL9ulLylEruskM/SyWPW/Icuu8pK59nKqZV/lmxfNnhnz7LlB6+7y4tv288dVf/2cgbuILkG9KCDgcnnwDoqBT2KoUunzBXdblKodCqEtwqtmc5anjsoWmV71cNjRUBz6eeyiGycoB0+E6r3FtXLQA6a6iBUWMA/lXb9++A6XE55doPLr+dTsaUBxnjPhxwKv/uLj1833hqWOZQYPKuo2qYqhqcqgQt7+M8Rw5jPtv0VPq90rLGU74vxiOpYp88h8NbxX64yw7X4v29UsAz7hra59DyNbafW3K7JXfJKf9T2Z9VvSfNsnwDjk+oO7Rs5V40y/INUBXLVuVwSc7g8h4yZ3D5rWqoqyl8+dRQQ02VA2W136sGxcrTa1pX5f2vKQBW7lWsFFp9gik9fsCpQHA5k1lW+ad2/kxHVxFgKnqqKkKbcVcJRZV6vCoHvcrBy1R6bOXgVTk8VQ6KPiHNXSVslVV6fJXH1jTNZ30Vjzu0bOV6qu5b5W1Xvm+q1HP0Rqzlcqg9q3oYqhygKt8qB7Yae9RqGPq0nL69iE6XqvXi1dTz5g2bh9ZXY2/dEcJq1SHqyoHVsuQdwq227kqPqXq8n/d4vhqGu6sGYYIhxDsiAoFlHfr0HuTvSs5cFT1elUORN0B5qoecmgKWN+xVCkU+PWmVQ9ZResmMp7wXzF1SHr7KiisFOFP98VUD25F6Ar3TqoQ9n32tso7K++8zv4bA6FPLcYRBlUmn6NC7s1rVodlqQ7E1HHfnDUhVb1XCWeVA5u2NrDqUax0Oa5ZVvWeuck+kKge5mkJs5fU4Dge6il7JqsGt2rBxxe1o4a/ysYLOKj2k9jrxg+ACnA0q/2NViL+rCQzeMFill6ymkOTT81ZpGZkjBzGf4dsaQlLlIeOKY+gqavLWUMPwskz5chU/Ky97xKHfyvvirr6v3gDr00BV1luxT2VVjvcr9e3trNjmsY7x8w4FF9fN3xOHVApQPqHsUDCqCDmp06QuI/1SIcEFAE6ETxgM9nc1gcfnGL8qwa/aUKj78BCwT2+bORyWvOGyclhz+y7v7Ymr1JPo7QWs0uNWEcxMlWV8egErBUSpSm2VevaqhcjKJ4GUHGG/qwS9Iw6BH2cgrFzL0fgxMAZMcDnlp0MDAE4fn2P86CWsU1VP/KgahnzCS9Wes0M/oxL9Vj6nQwMAAL/jSxYBAEDAIbgAAADbILgAAADbILgAAADbILgAAADbILgAAADbILgAAADbCJjgkpaWpuTkZKWkpPi7FAAAcIpwAToAAOB3XIAOAAAEHIILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwjYAJLlw5FwCAwMeVcwEAgN9x5VwAABBwCC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2Aia48CWLAAAEPr5kEQAA+B1fsggAAAIOwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANhGwASXtLQ0JScnKyUlxd+lAACAU8Qyxhh/F1GXcnNzFR0drZycHEVFRfm7HAAAUAu1ff8OmB4XAAAQ+AguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANs7I4PLxxx/rnHPOUVJSkl5++WV/lwMAAM4QLn8XUFVZWZkmTZqkBQsWKDo6Wt26ddOwYcMUGxvr79IAAICfnXE9LsuXL1fHjh3VpEkTRUREaNCgQZo7d66/ywIAAGeAOg8uixYt0pAhQ5SYmCjLsvTBBx9UWyYtLU0tW7ZUaGioevbsqeXLl3vn7dq1S02aNPHeb9KkiXbu3FnXZQIAABuq8+BSUFCgzp07Ky0trcb5s2fP1qRJk/T3v/9dq1evVufOnTVgwADt2bPnhLZXXFys3NxcnxsAAAhMdR5cBg0apEceeUTDhg2rcf6zzz6rW265RePGjVNycrJeeuklhYWF6ZVXXpEkJSYm+vSw7Ny5U4mJiUfc3uOPP67o6GjvrVmzZnW7QwAA4IxxWo9xKSkp0apVq5Samnq4AIdDqampWrp0qSSpR48eWrdunXbu3Kn8/Hx99tlnGjBgwBHXef/99ysnJ8d7y8jIOOX7AQAA/OO0nlW0b98+ud1uxcXF+UyPi4vTxo0bywtyufTMM8+oX79+8ng8uueee456RlFISIhCQkJOad0AAODMcMadDi1JV1xxha644gp/lwEAAM4wp3WoqGHDhnI6ncrKyvKZnpWVpfj4+NNZCgAAsKHTGlyCg4PVrVs3zZ8/3zvN4/Fo/vz56tWr10mtOy0tTcnJyUpJSTnZMgEAwBmqzoeK8vPztWXLFu/9bdu2KT09XQ0aNFDz5s01adIkjRkzRt27d1ePHj00ffp0FRQUaNy4cSe13fHjx2v8+PHKzc1VdHT0ye4GAAA4A9V5cFm5cqX69evnvT9p0iRJ0pgxYzRr1iyNGDFCe/fu1YMPPqjMzEx16dJFn3/+ebUDdgEAAKqyjDHG30XUpYoel5ycHEVFRfm7HAAAUAu1ff8+476rCAAA4EgILgAAwDYCJrhwVhEAAIGPY1wAAIDfcYwLAAAIOAQXAABgGwQXAABgGwQXAABgGwETXDirCACAwMdZRQAAwO84qwgAAAQcggsAALANggsAALANggsAALANggsAALCNgAkunA4NAEDg43RoAADgd5wODQAAAg7BBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQAA2EbABBcuQAcAQODjAnQAAMDvuAAdAAAIOAQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwQXAABgGwETXLhyLgAAgY8r5wIAAL/jyrkAACDgEFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtEFwAAIBtuPxdAADgzOJ2u1VaWurvMhBggoKC5HQ6T3o9BBcAgCTJGKPMzExlZ2f7uxQEqJiYGMXHx8uyrBNeR8AEl7S0NKWlpcntdvu7FACwpYrQ0rhxY4WFhZ3UmwtQmTFGhYWF2rNnjyQpISHhhNfFlywCAOR2u7V582Y1btxYsbGx/i4HAWr//v3as2eP2rVrV23YiC9ZBADUWsUxLWFhYX6uBIGs4vl1MsdQEVwAAF4MD+FUqovnF8EFAADYBsEFAADYBsEFAIAqWrZsqenTp/u7DNSA4AIAsC3Lso56mzp16gmtd8WKFbr11ltPqra+fftq4sSJJ7UOVBcw13EBAJx9du/e7f199uzZevDBB7Vp0ybvtIiICO/vxhi53W65XMd+62vUqFHdFoo6Q48LAKBGxhgVlpT55VbbS4zFx8d7b9HR0bIsy3t/48aNioyM1GeffaZu3bopJCRE33zzjbZu3aqhQ4cqLi5OERERSklJ0Zdffumz3qpDRZZl6eWXX9awYcMUFhampKQkffTRRyfVvu+99546duyokJAQtWzZUs8884zP/BdeeEFJSUkKDQ1VXFycrr76au+8d999V506dVK9evUUGxur1NRUFRQUnFQ9dkGPCwCgRgdL3Up+8Au/bHv9QwMUFlw3b1H33Xefnn76abVu3Vr169dXRkaGLr/8cj366KMKCQnRa6+9piFDhmjTpk1q3rz5Edczbdo0Pfnkk3rqqaf0/PPPa9SoUdq+fbsaNGhw3DWtWrVK1157raZOnaoRI0ZoyZIluuOOOxQbG6uxY8dq5cqVuvPOO/Wf//xHvXv31oEDB7R48WJJ5b1MI0eO1JNPPqlhw4YpLy9PixcvrnXYszuCCwAgoD300EO69NJLvfcbNGigzp07e+8//PDDmjNnjj766CNNmDDhiOsZO3asRo4cKUl67LHH9Nxzz2n58uUaOHDgcdf07LPPqn///poyZYokqV27dlq/fr2eeuopjR07Vjt27FB4eLh+//vfKzIyUi1atFDXrl0llQeXsrIyXXXVVWrRooUkqVOnTsddg10RXAAANaoX5NT6hwb4bdt1pXv37j738/PzNXXqVH3yySfeEHDw4EHt2LHjqOs577zzvL+Hh4crKirK+907x2vDhg0aOnSoz7QLL7xQ06dPl9vt1qWXXqoWLVqodevWGjhwoAYOHOgdpurcubP69++vTp06acCAAbrssst09dVXq379+idUi91wjAsAoEaWZSks2OWXW11ewTc8PNzn/l/+8hfNmTNHjz32mBYvXqz09HR16tRJJSUlR11PUFBQtfbxeDx1VmdlkZGRWr16td58800lJCTowQcfVOfOnZWdnS2n06l58+bps88+U3Jysp5//nmdc8452rZt2ymp5UxDcAEAnFW+/fZbjR07VsOGDVOnTp0UHx+vX3755bTW0KFDB3377bfV6qr85YMul0upqal68skn9cMPP+iXX37RV199Jak8NF144YWaNm2avv/+ewUHB2vOnDmndR/8haEiAMBZJSkpSe+//76GDBkiy7I0ZcqUU9ZzsnfvXqWnp/tMS0hI0J///GelpKTo4Ycf1ogRI7R06VLNmDFDL7zwgiTp448/1s8//6yLLrpI9evX16effiqPx6NzzjlHy5Yt0/z583XZZZepcePGWrZsmfbu3asOHTqckn040xBcAABnlWeffVY33nijevfurYYNG+ree+9Vbm7uKdnWG2+8oTfeeMNn2sMPP6y//e1vevvtt/Xggw/q4YcfVkJCgh566CGNHTtWkhQTE6P3339fU6dOVVFRkZKSkvTmm2+qY8eO2rBhgxYtWqTp06crNzdXLVq00DPPPKNBgwadkn0401gmwM6fys3NVXR0tHJychQVFeXvcgDAFoqKirRt2za1atVKoaGh/i4HAepoz7Pavn9zjAsAALCNgAkuaWlpSk5OVkpKir9LAQAAp0jABJfx48dr/fr1WrFihb9LAQAAp0jABBcAABD4CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AAMA2CC4AgLNe3759NXHiRO/9li1bavr06Ud9jGVZ+uCDD05623W1nrMFwQUAYFtDhgzRwIEDa5y3ePFiWZalH3744bjXu2LFCt16660nW56PqVOnqkuXLtWm7969+5R/z9CsWbMUExNzSrdxuhBcAAC2ddNNN2nevHn69ddfq82bOXOmunfvrvPOO++419uoUSOFhYXVRYnHFB8fr5CQkNOyrUBAcAEA1MwYqaTAP7dafv/v73//ezVq1EizZs3ymZ6fn6933nlHN910k/bv36+RI0eqSZMmCgsLU6dOnfTmm28edb1Vh4p++uknXXTRRQoNDVVycrLmzZtX7TH33nuv2rVrp7CwMLVu3VpTpkxRaWmppPIej2nTpmnNmjWyLEuWZXlrrjpUtHbtWl1yySWqV6+eYmNjdeuttyo/P987f+zYsbryyiv19NNPKyEhQbGxsRo/frx3Wydix44dGjp0qCIiIhQVFaVrr71WWVlZ3vlr1qxRv379FBkZqaioKHXr1k0rV66UJG3fvl1DhgxR/fr1FR4ero4dO+rTTz894VqOxXXK1gwAsLfSQumxRP9s+4FdUnD4MRdzuVy64YYbNGvWLE2ePFmWZUmS3nnnHbndbo0cOVL5+fnq1q2b7r33XkVFRemTTz7R6NGj1aZNG/Xo0eOY2/B4PLrqqqsUFxenZcuWKScnx+d4mAqRkZGaNWuWEhMTtXbtWt1yyy2KjIzUPffcoxEjRmjdunX6/PPP9eWXX0qSoqOjq62joKBAAwYMUK9evbRixQrt2bNHN998syZMmOATzhYsWKCEhAQtWLBAW7Zs0YgRI9SlSxfdcsstx9yfmvavIrR8/fXXKisr0/jx4zVixAgtXLhQkjRq1Ch17dpVL774opxOp9LT0xUUFCSp/Ct3SkpKtGjRIoWHh2v9+vWKiIg47jpqi+ACALC1G2+8UU899ZS+/vpr9e3bV1L5MNHw4cMVHR2t6Oho/eUvf/Eu/6c//UlffPGF3n777VoFly+//FIbN27UF198ocTE8iD32GOPVTsu5W9/+5v395YtW+ovf/mL3nrrLd1zzz2qV6+eIiIi5HK5FB8ff8RtvfHGGyoqKtJrr72m8PDy4DZjxgwNGTJE//jHPxQXFydJql+/vmbMmCGn06n27dtr8ODBmj9//gkFl/nz52vt2rXatm2bmjVrJkl67bXX1LFjR61YsUIpKSnasWOH/vrXv6p9+/aSpKSkJO/jd+zYoeHDh6tTp06SpNatWx93DceD4AIAqFlQWHnPh7+2XUvt27dX79699corr6hv377asmWLFi9erIceekiS5Ha79dhjj+ntt9/Wzp07VVJSouLi4lofw7JhwwY1a9bMG1okqVevXtWWmz17tp577jlt3bpV+fn5KisrU1RUVK33o2JbnTt39oYWSbrwwgvl8Xi0adMmb3Dp2LGjnE6nd5mEhAStXbv2uLZVeZvNmjXzhhZJSk5OVkxMjDZs2KCUlBRNmjRJN998s/7zn/8oNTVV11xzjdq0aSNJuvPOO3X77bdr7ty5Sk1N1fDhw0/ouKLa4hgXAEDNLKt8uMYft0NDPrV100036b333lNeXp5mzpypNm3a6OKLL5YkPfXUU/rnP/+pe++9VwsWLFB6eroGDBigkpKSOmuqpUuXatSoUbr88sv18ccf6/vvv9fkyZPrdBuVVQzTVLAsSx6P55RsSyo/I+rHH3/U4MGD9dVXXyk5OVlz5syRJN188836+eefNXr0aK1du1bdu3fX888/f8pqIbgAAGzv2muvlcPh0BtvvKHXXntNN954o/d4l2+//VZDhw7V9ddfr86dO6t169bavHlzrdfdoUMHZWRkaPfu3d5p3333nc8yS5YsUYsWLTR58mR1795dSUlJ2r59u88ywcHBcrvdx9zWmjVrVFBQ4J327bffyuFw6Jxzzql1zcejYv8yMjK809avX6/s7GwlJyd7p7Vr105333235s6dq6uuukozZ870zmvWrJluu+02vf/++/rzn/+sf//736ekVongAgAIABERERoxYoTuv/9+7d69W2PHjvXOS0pK0rx587RkyRJt2LBBf/zjH33OmDmW1NRUtWvXTmPGjNGaNWu0ePFiTZ482WeZpKQk7dixQ2+99Za2bt2q5557ztsjUaFly5batm2b0tPTtW/fPhUXF1fb1qhRoxQaGqoxY8Zo3bp1WrBggf70pz9p9OjR3mGiE+V2u5Wenu5z27Bhg1JTU9WpUyeNGjVKq1ev1vLly3XDDTfo4osvVvfu3XXw4EFNmDBBCxcu1Pbt2/Xtt99qxYoV6tChgyRp4sSJ+uKLL7Rt2zatXr1aCxYs8M47FQguAICAcNNNN+m3337TgAEDfI5H+dvf/qbzzz9fAwYMUN++fRUfH68rr7yy1ut1OByaM2eODh48qB49eujmm2/Wo48+6rPMFVdcobvvvlsTJkxQly5dtGTJEk2ZMsVnmeHDh2vgwIHq16+fGjVqVOMp2WFhYfriiy904MABpaSk6Oqrr1b//v01Y8aM42uMGuTn56tr164+tyFDhsiyLH344YeqX7++LrroIqWmpqp169aaPXu2JMnpdGr//v264YYb1K5dO1177bUaNGiQpk2bJqk8EI0fP14dOnTQwIED1a5dO73wwgsnXe+RWMbU8mR5m8jNzVV0dLRycnKO+6AoADhbFRUVadu2bWrVqpVCQ0P9XQ4C1NGeZ7V9/6bHBQAA2AbBBQAA2AbBBQAA2AbBBQAA2AbBBQDgFWDna+AMUxfPL4ILAMB7JdbCwkI/V4JAVvH8qnrl3+PBdxUBAOR0OhUTE6M9e/ZIKr+eiHWcl90HjsQYo8LCQu3Zs0cxMTE+37N0vM7I4DJs2DAtXLhQ/fv317vvvuvvcgDgrFDxrcUV4QWoazExMUf9duzaOCODy1133aUbb7xRr776qr9LAYCzhmVZSkhIUOPGjVVaWurvchBggoKCTqqnpcIZGVz69u2rhQsX+rsMADgrOZ3OOnmDAU6F4z44d9GiRRoyZIgSExNlWZY++OCDasukpaWpZcuWCg0NVc+ePbV8+fK6qBUAAJzljrvHpaCgQJ07d9aNN96oq666qtr82bNna9KkSXrppZfUs2dPTZ8+XQMGDNCmTZvUuHFjSVKXLl1UVlZW7bFz5871+WKs2iguLvb5hs3c3Nzj3CMAAGAXxx1cBg0apEGDBh1x/rPPPqtbbrlF48aNkyS99NJL+uSTT/TKK6/ovvvukySlp6efWLU1ePzxx73fUAkAAAJbnV7HpaSkRKtWrVJqaurhDTgcSk1N1dKlS+tyU17333+/cnJyvLeMjIxTsh0AAOB/dXpw7r59++R2uxUXF+czPS4uThs3bqz1elJTU7VmzRoVFBSoadOmeuedd9SrV68alw0JCVFISMhJ1Q0AAOzhjDyr6Msvv/R3CQAA4AxUp0NFDRs2lNPpVFZWls/0rKysk77gDAAAQJ0Gl+DgYHXr1k3z58/3TvN4PJo/f/4Rh3rqSlpampKTk5WSknJKtwMAAPznuIeK8vPztWXLFu/9bdu2KT09XQ0aNFDz5s01adIkjRkzRt27d1ePHj00ffp0FRQUeM8yOlXGjx+v8ePHKzc3V9HR0ad0WwAAwD+OO7isXLlS/fr1896fNGmSJGnMmDGaNWuWRowYob179+rBBx9UZmamunTpos8//7zaAbsAAADHyzLGGH8XUZcqelxycnIUFRXl73IAAEAt1Pb9u06PcQEAADiVCC4AAMA2Aia4cFYRAACBj2NcAACA33GMCwAACDgEFwAAYBsEFwAAYBsEFwAAYBsBE1w4qwgAgMDHWUUAAMDvOKsIAAAEHIILAACwDYILAACwDYILAACwDYILAACwjYAJLpwODQBA4ON0aAAA4HecDg0AAAIOwQUAANgGwQUAANgGwQUAANgGwQUAANgGwQUAANhGwAQXruMCAEDg4zouAADA77iOCwAACDgEFwAAYBsEFwAAYBsEFwAAYBsEFwAAYBsEFwAAYBsEFwAAYBsEFwAAYBsEFwAAYBsBE1y45D8AAIGPS/4DAAC/45L/AAAg4BBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbRBcAACAbQRMcOFLFgEACHx8ySIAAPA7vmQRAAAEHIILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwDYILAACwjYAJLmlpaUpOTlZKSoq/SwEAAKeIZYwx/i6iLuXm5io6Olo5OTmKiorydzkAAKAWavv+HTA9LgAAIPARXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG0QXAAAgG2cccElIyNDffv2VXJyss477zy98847/i4JAACcIVz+LqAql8ul6dOnq0uXLsrMzFS3bt10+eWXKzw83N+lAQAAPzvjgktCQoISEhIkSfHx8WrYsKEOHDhAcAEAAMc/VLRo0SINGTJEiYmJsixLH3zwQbVl0tLS1LJlS4WGhqpnz55avnz5CRW3atUqud1uNWvW7IQeDwAAAstx97gUFBSoc+fOuvHGG3XVVVdVmz979mxNmjRJL730knr27Knp06drwIAB2rRpkxo3bixJ6tKli8rKyqo9du7cuUpMTJQkHThwQDfccIP+/e9/H7We4uJiFRcXe+/n5uYe7y4BAACbsIwx5oQfbFmaM2eOrrzySu+0nj17KiUlRTNmzJAkeTweNWvWTH/6059033331Wq9xcXFuvTSS3XLLbdo9OjRR1126tSpmjZtWrXpOTk5ioqKqv3OAAAAv8nNzVV0dPQx37/r9KyikpISrVq1SqmpqYc34HAoNTVVS5curdU6jDEaO3asLrnkkmOGFkm6//77lZOT471lZGSccP0AAODMVqfBZd++fXK73YqLi/OZHhcXp8zMzFqt49tvv9Xs2bP1wQcfqEuXLurSpYvWrl17xOVDQkIUFRXlcwMAAIHpjDurqE+fPvJ4PP4uAwAAnIHqtMelYcOGcjqdysrK8pmelZWl+Pj4utwUAAA4C9VpcAkODla3bt00f/587zSPx6P58+erV69edbmpatLS0pScnKyUlJRTuh0AAOA/xz1UlJ+fry1btnjvb9u2Tenp6WrQoIGaN2+uSZMmacyYMerevbt69Oih6dOnq6CgQOPGjavTwqsaP368xo8f7z0qGQAABJ7jDi4rV65Uv379vPcnTZokSRozZoxmzZqlESNGaO/evXrwwQeVmZmpLl266PPPP692wC4AAMDxOqnruJyJanseOAAAOHP45TouAAAApxLBBQAA2EbABBfOKgIAIPBxjAsAAPA7jnEBAAABh+ACAABsg+ACAABsg+ACAABsI2CCC2cVAQAQ+DirCAAA+B1nFQEAgIBDcAEAALZBcAEAALZBcAEAALZBcAHqiDFGJWUef5cB2EZRqVsHS9z+LgM2EzDBhdOh4U8ZBwo16J+L1e/phdqdc9Df5aAWPB6jOd//qnveXaOd2fzNTreC4jJd/s/FuvipBdqTV1Sn6y5ze/T6su1atf1Ana4XZwZOhwZO0qbMPN3wyjJl5RZLkvqe00gzx6bIsiw/V4YjWbJlnx79dIN+3JUrSerSLEbv3NZLQc6A+Sx3xnv44/X6/77ZJkka0jlRz4/sWmfrfuTj9Xr5m20Kdjo0+48XqGvz+nW2bpw6nA59GuzNK9ZvBSX+LsP28ovLVFBc5u8yTsiq7Qd07b+WKiu3WK0bhSvY5dDCTXv1zqpf/V0aarA5K0/jZi7XH15eph935SoyxKWIEJfSM7L1/Fdb/F3eWWPdzhzN/LY8tFiW9L81u7Rw0546Wfe7q37Vy4cCUYnbo9v+u6rOe3TOFgXFZZrx1U/6fN1unUl9HASX45RdWKI3l+/Qdf+3VD0e+1K/e3KBvtqYddrr2JtXrM/W7tbGzNzTvu26NH9Dlno9Pl+9n/hKS7bu83c5R7Q756BWbT+grXvzdaCgRG6P0YJNezTq5WXKOViq85vH6P3be2vSpe0klX+aPJuGjDbsztXQGd/ozje/V15Rqb/LqSbnYKmmfvSjBv1zsRZs2iuXw9LY3i218K999dhVnSRJM776iaGF08DtMXpgzlp5jPT78xJ044WtJElTPlx30se7rN7xmx54f60k6ZbftVJS4whl5Rbr9v+uVnFZYBxLk11Yog/Tdyrn4Kl9na3JyNbg5xbr6bmbddt/V+vmV1dq1xkypMpQUS198WOm3lmZoa8371Wp27fJLEt6YFAH3fy7VnU2PLAnt0g5B0tlJFX8hfbmFWvxlr1avHmf1u/O9W77jxe10d2XJinE5ayTbZ8OHo/RjAVb9P++3OzdP5fD0iNXnqvrejQ/LTXkFJZqY2autuzNV6vYcPVqE1vt71dS5tGMBVv0woItKvMc/rtXLGaMdHG7Rnrx+vMVFuyS22M0/MUlSs/IVr9zGumV0zRklFdUqv98t11BDoeuv6CF6gXX3XPh6817lbZgizomRumu/kmKCQv2mf/Z2t2a9PYaHSwtf2NIahyh/29MiprHhh1z3cYYHSx1q16Q85S0k8dj9N7qX/WPzzdqX3557+hlyXG6b1B7tW4U4V3u7tnpmvP9TjVrUE+f3vk7RYYGHXW9GzNzVVjiVtdmMbWu+7O1u/XU3E3q2qy+Hri8vWIjQk58x2zs1SW/6O8f/ajIEJfm//lihYe4dOmzX2tXTpFuu7iN7hvU/oTWm5lTpCEzvtHevGJdlhynl67vpu0HCnXFjG+UV1SmkT2a6/FDIdWutuzJ102vrtD2/YVqWr+eZvzhfHVpFlOn23B7jP61aKuenbtZZR6jRpEhyi4sUanbKCLEpXsHtdeoHs3lcNT967W2798El1qa9Ha63l+9U5LUPj5SQ7s00cBz4/Wvr7fqrRUZkqRrujXVI8POPeEAsT+/WJ+s3a0Pvt+p1Tuyj7l864bh+nlfgSSpQ0KUpo/oonPiI09o26dTfnGZJs1O19z15T1Voy9ooZyDpfpozS5J5Z+U7hvUQc6TfGGUlHm0KTNPu3IOam9ecfktv1g7fzuoTZl5ysz17T7u1CRa4/u10WXJ8XI4LK3JyNY97/6gTVl5kqSE6FDlF5cpr+jwsNawrk30j+HnKdh1uPNyy548Xf7cNyop8+ipq8/TNd2bndR+HGsf31i2Xc9/tUX7Dw1bxkWF6M+XnqPh3ZqeVBtmHCjUwx+v9/6dJCm6XpDuTk3SqAtayGlZmv7lZj13aIilZ6sG2ravQHvyihUTFqQXRp2v3m0a1rjuolK3PlqzS68t/UXrduaqW4v6mpiapD5tG9ZJgPF4jL77eb+enrvJ+1pq3Shc067oqN8lNaq2fG5RqQZNX6yd2Qc1/PymeubazjWud01Gtv7fl5u1cNNeSeXPmQmXtNWlHeKO+I88p7BUD360Th+m7/JOqx8WpCm/T9awrk3O+GOhikrdOlBQooTo0JOuNSu3SP2f+Vr5xWV6eGhHje7VUpL05fos3fzaSjkdlj7+Ux91SDi+/91FpW6N+NdSrfk1R+fEReq9O3orIsQlSVqwaY9unLVCxkiPDjtXo3q2OKl9qC23xygzt0g79hcq40ChdhwoVInbo0vaN1aPlg2O+43/m5/26fbXV/n8/3E5LN03qL1u6lM3H5q37y/Qfe+t1dKf90uSLu8Ur8eHnac9eUW6970fvK+lHi0b6PHhndSmUvivCwSXOg4uy7cd0KLNe3VFl0S1izscDowxmvntL3rkk/XyGKl7i/oa2rWJXA6r/OYsfzIVlXpUVOpWUalHxWVuuT1GHmPkMZLHGG3OzNPin/Z5P9VblhRTL+jQ75YsSfWCnerRqoEubtdIF7ZtqIYRIfrix0zd//5aHSgoUbDTobtSk3ROXKSKy8q3U1TqUZnHI4/n8LYq/uKWdXjdQU5LIS6nQoIc3p+OKi8Ec+ixlWt3e4zKPB6Vuo3K3B55TPmLKchlyeVwKMhpqcRtVHDoOJb84jJ9/MNubdmTr2CnQ49cea6uTWkmY4yem1/eAyNJl7RvrAEd43SwxK2Dh9quzHP4VGNLliyrvE0iQ1yKDA1SRIhLBSVlSs/IVnpGtn7clXvM05ObxNRTq4bhWrX9N2+PQdvGEerWvL7eWZUhj5Fiw4P10NBzdXmneFmWpVK3R9mFpXJ7jOKjQ2tc70tfb9UTn21UZKhLL47qpuh6QYfatrxdi0rdOnjo+XCw1K3iUvehv9nh50eIy6HQIKdCXE6FBjkU7HIo2Fn+M8jp0OasPD0zd7N2HCiUVB5ki8s83jNkzomL1F8GnKPmDcJUWFKmgyVuFZa4tTe/WL/sL9CO/YX6ZX+hdmUfVHxUqNonRKp9fJTaJ0QqfUe2Xvp6q4rLPHI6LF2X0kyrtv+mjZl53jZqElNPX28ufwO/8cJWeuDy9tpfUKJbX1upNb/myOWwNHlwB/Vo1UAej+Q2RqVuj75cn6XZKzOUXVi9q7t7i/qamNpOF7at3vtljFFBiVt5RaXKKyqTMVKjyBDVDwvyLvvrb4V6b9VOvbs6QxkHytshLNipu/onadyFrXwCZlUrfjmgEf9aKo+RHvx9srq1qK/wEKfCgl3ak1esGV/9pC83lB+H4XRYCnJaKiotf361j4/U+H5t1aNVA0XXC1JoUPmHlwWb9ui+935QVm6xHJY0pndLLd2639uOv0tqqIeGnqvGkSHe15QxRm6PkdsYeTyHXrOSQlwO1QtyKjTIKafDkjFGhSXuQ2G6VAXFbjksSw6H5HI45Dz0M9hV/rwLdh1+Dh3tTc7tMfpxV44W/7RP327Zp5Xbf1NJmUfxUaHq1SZWvVrHqlebWDVrUHOPWpnbo98KS/VbYYkcltQgPEQx9YLkcFi64/VV+nRtpro0i9F7t/f2Cda3/3eVPltXPu//jeii737e773lFZWpe8sG6tU6Vr3bxKpjYpQOFJTou20H9N3P+/XNT/u040ChYsKC9NH4PtV6+15YuEVPfr5JLoelXm1i1TExWh0To9QxMUpN64cpyGnVyRt/TmGpvtqUpbk/ZunrzXtVeIShr7ioEA3ulKjB5yUo2OnQ+t052rA7T+t35WpvfrG6t6ivfu0bq09SQ0WFBumNZTs05cN1cnuMureor6ev6aynvtikT9buliT1b99YTww/Tw0jgo97PwpLyvT5uky9s/JXb2CpF+TUtCs66pruTb3rc3uMXlv6i576YpMKS9y6o28b3TPwxHrHjoTgcprPKvp6815NeGO1Txo+EZ2aRGtol0QN6ZyouKia3xSr2ptXrPve+0HzN9bNwW2nQ1xUiF66vlu1o/0/WrNLf3lnTZ1dDyUmLEgtY8PVKDKk/BYRovjoULWLi1BSXKSiDg0J7M8v1qwlv2jWkl98/oZDuyTq70M6qkF48JE2UaMyt0fDX1qqNRnZdbIfR9MoMkQTU5N0bfdmcnuM/vtdeQ9MXYyB924Tq6lXdFS7uEiVuT16a0WGnpm7Sb8dCh3BLoceG9ZJV3dr6n1MUalb97z7g7cH7UiaxNTT6F4tdEn7xnpz+Q69vmyH9++eEB0qS1KppzwQl7mNCkrK5Knhv1Ww06FGkSGKDHVpU1aeN5hHhrh0RZdE/emSpCMGzKqembvpqAfpOixpWNemurN/W0WEuPT/fbNNry3drvwqB5eHBjkUFRqkPXnlZ5q1bhSuZ67prK7N66vU7dG/F/+s6V/+dMLP8yCndegDxPE/1mFJoUFObwhyOS2VlnlU4i4PlgdL3dXqsqzDQ9YVnA7r0Doc3jCVXVha4/POYUn1w4K1v6BEToel/03oo+RE3//PmTlFSn3262ptWZPQIIc3NFYID3bq32O619jLZ4zR3bPT9UF6zc/Jw/viVL3gig8ITgU7LQW7yoNemdujMo9RqdvI7fGoXrBLUaEuRYa6FBkSpJ3ZB/Xdz/t9hpRdDktN69dT89hwNW9QTwdLPJq7PrPW7xMuh6V2cZHeQwOu7JKoJ4afp9Agp4wxemP5Dk3733rv38vpsBQW7FR4sEvhIU6Fh7i898NCXApxOVQRayxLKihx6+tNe71tbllSn7YNNe2Kjj5DqZX9+luh0hZs0d+HdPQG9Lpy1gWXtLQ0paWlye12a/PmzX45HXrr3nz9e9HPyjlYWt4D4Sn/hyuVv9BCgpwKPfTp2eUoT/hOhyWHJcWEBWtAx3i1bXxiXW/GGL29MkNvLi8ftqr4lBXicirIacnhsOSwLDkP9bJUPMZjJKPyN9rKvTTFZe5q/6ik8heGVWk9FZ88XY7D++T2lL/AS8rKfwY5LUWEuA69iFxqFBmi6y9orsaRNb+ZpGdk66WFW1Xq9ig02On95+hyOHz+gVZ8As8vKlNecanyi8rkcFg6r0m0ujavry7NYtQiNuy4PoFUHCuyZMt+je3dUqnJccf1d6js5735+uu7P+hAQYmKS90qKvOouNQttzGH/0kGOcufF0GH/14hLoecDkslZR4VHfp7FB16Myl1e1Ry6E0m2Gnpuh7NdfPvWiks2OWz7ZzCUr2wcIveXfWrjMp7HcKCnaoX7FL9Q2GuRWyYWsSGKSG6nnbnHNSG3XnamJmnjbtz5bAs3dk/ydvLVHXdz3/1k9b8mq0HLu9Q46mmxhj936Kf9eqSX1TqMXJWPNcdUttGERrVs4X6tW/s84k7K7dILy7cqjeW7zjqG7rTYSky1CVL8gaoynq3idU13ZtqYMeE4z7Wp9Tt0cMfr9fSrfu9vRkVZ7wNPi9Bd/ZPqtY9nl1YollLftHsFRnKyi2qFiZuvLCV7hl4TrV/8tv2FehvH6zVt1v211iLZenQa7a8jUrcNbeJ01H++go7tK9uz+Eem/JA4ql2XN6xRIa4dEGbWPVp21AXtm2oJjH1tHrHb1q6db+WbN2nH37N8XmDrqn2mHpBcnuMcqu8Sf/x4ta6f1CHGh/35vIduv/9tQpyWurarL4uaBOrC1o3UEy9YH33834t2bpfy7aV98BYlpScEKVerWN1QetYpRzq7ToSY4zW7szR2p05+nFXrn7clauNu3NVXMcXjWwXF6HLkuN1Wcc4dUyMrjZcW1zm1qLN+/TxD7v05fosBbkcSk6IKr8lRql+WLC+3bJPX23ao5/3FngfN+nSdvrTJW2rvR7X78rV3bPTvUPaJ6J5gzBd3a2phndrqiYx9U54PSfrrAsuFbiOC2Bv+/OL9cv+wsOB2Fk+7BpxaEgwNOjwUEdJmUd784uVlVuk/fklah8fecQhjJPh8ZhaHZPg8Rjll5Qpp7BU2YWligkLOmY9FWfSVAQVh3X4A0LVdVcOsi6HVa09jlZXiduj4tKKMHx4qLLM7fEOPwY5ywN0QnSoXEe5pk1RqVs5B0t1sMStorLyq9+WeYxi6gWpQXiwYsKCvW/YpW6Pfiso0b78EhWVudWlacxR2zLjQKEaRoQcMXS6PUZb9+YrLjJU0WFHP4j6WNweo/ziMu8VfA8eapeSMo/PBwWPkVxOS8HO8uej07K8w5a5B8uHLusFO9W/Q5xaNQyv9fYr3n6P9Pfbvr9Ai3/ap+YNwnRRu+rHZlVeT0GJ2zskX1BcHroPlpb/XlhS/rO4zKPygcfyD4CWJZ3fvP4JHXNzKhBcCC4AANgGF6ADAAABh+ACAABsg+ACAABsg+ACAABsg+ACAABsg+ACAABsg+ACAABsg+ACAABsI2CCS1pampKTk5WSkuLvUgAAwCnClXMBAIDfceVcAAAQcAguAADANgguAADANlz+LqCuVRyyk5ub6+dKAABAbVW8bx/r0NuACy55eXmSpGbNmvm5EgAAcLzy8vIUHR19xPkBd1aRx+PRrl27FBkZKcuy6my9ubm5atasmTIyMjhb6TSgvU8f2vr0oa1PH9r69KmrtjbGKC8vT4mJiXI4jnwkS8D1uDgcDjVt2vSUrT8qKooXwWlEe58+tPXpQ1ufPrT16VMXbX20npYKHJwLAABsg+ACAABsg+BSSyEhIfr73/+ukJAQf5dyVqC9Tx/a+vShrU8f2vr0Od1tHXAH5wIAgMBFjwsAALANggsAALANggsAALANggsAALANggsAALANgkstpaWlqWXLlgoNDVXPnj21fPlyf5dke48//rhSUlIUGRmpxo0b68orr9SmTZt8likqKtL48eMVGxuriIgIDR8+XFlZWX6qOHA88cQTsixLEydO9E6jrevOzp07df311ys2Nlb16tVTp06dtHLlSu98Y4wefPBBJSQkqF69ekpNTdVPP/3kx4rtye12a8qUKWrVqpXq1aunNm3a6OGHH/b5kj7a+sQsWrRIQ4YMUWJioizL0gcffOAzvzbteuDAAY0aNUpRUVGKiYnRTTfdpPz8/JMvzuCY3nrrLRMcHGxeeeUV8+OPP5pbbrnFxMTEmKysLH+XZmsDBgwwM2fONOvWrTPp6enm8ssvN82bNzf5+fneZW677TbTrFkzM3/+fLNy5UpzwQUXmN69e/uxavtbvny5admypTnvvPPMXXfd5Z1OW9eNAwcOmBYtWpixY8eaZcuWmZ9//tl88cUXZsuWLd5lnnjiCRMdHW0++OADs2bNGnPFFVeYVq1amYMHD/qxcvt59NFHTWxsrPn444/Ntm3bzDvvvGMiIiLMP//5T+8ytPWJ+fTTT83kyZPN+++/bySZOXPm+MyvTbsOHDjQdO7c2Xz33Xdm8eLFpm3btmbkyJEnXRvBpRZ69Ohhxo8f773vdrtNYmKiefzxx/1YVeDZs2ePkWS+/vprY4wx2dnZJigoyLzzzjveZTZs2GAkmaVLl/qrTFvLy8szSUlJZt68eebiiy/2Bhfauu7ce++9pk+fPkec7/F4THx8vHnqqae807Kzs01ISIh58803T0eJAWPw4MHmxhtv9Jl21VVXmVGjRhljaOu6UjW41KZd169fbySZFStWeJf57LPPjGVZZufOnSdVD0NFx1BSUqJVq1YpNTXVO83hcCg1NVVLly71Y2WBJycnR5LUoEEDSdKqVatUWlrq0/bt27dX8+bNafsTNH78eA0ePNinTSXaui599NFH6t69u6655ho1btxYXbt21b///W/v/G3btikzM9OnraOjo9WzZ0/a+jj17t1b8+fP1+bNmyVJa9as0TfffKNBgwZJoq1Pldq069KlSxUTE6Pu3bt7l0lNTZXD4dCyZctOavsB9+3QdW3fvn1yu92Ki4vzmR4XF6eNGzf6qarA4/F4NHHiRF144YU699xzJUmZmZkKDg5WTEyMz7JxcXHKzMz0Q5X29tZbb2n16tVasWJFtXm0dd35+eef9eKLL2rSpEl64IEHtGLFCt15550KDg7WmDFjvO1Z0/8U2vr43HfffcrNzVX79u3ldDrldrv16KOPatSoUZJEW58itWnXzMxMNW7c2Ge+y+VSgwYNTrrtCS44I4wfP17r1q3TN9984+9SAlJGRobuuusuzZs3T6Ghof4uJ6B5PB51795djz32mCSpa9euWrdunV566SWNGTPGz9UFlrfffluvv/663njjDXXs2FHp6emaOHGiEhMTaesAxlDRMTRs2FBOp7Pa2RVZWVmKj4/3U1WBZcKECfr444+1YMECNW3a1Ds9Pj5eJSUlys7O9lmetj9+q1at0p49e3T++efL5XLJ5XLp66+/1nPPPSeXy6W4uDjauo4kJCQoOTnZZ1qHDh20Y8cOSfK2J/9TTt5f//pX3XfffbruuuvUqVMnjR49Wnfffbcef/xxSbT1qVKbdo2Pj9eePXt85peVlenAgQMn3fYEl2MIDg5Wt27dNH/+fO80j8ej+fPnq1evXn6szP6MMZowYYLmzJmjr776Sq1atfKZ361bNwUFBfm0/aZNm7Rjxw7a/jj1799fa9euVXp6uvfWvXt3jRo1yvs7bV03Lrzwwmqn9W/evFktWrSQJLVq1Urx8fE+bZ2bm6tly5bR1sepsLBQDofv25jT6ZTH45FEW58qtWnXXr16KTs7W6tWrfIu89VXX8nj8ahnz54nV8BJHdp7lnjrrbdMSEiImTVrllm/fr259dZbTUxMjMnMzPR3abZ2++23m+joaLNw4UKze/du762wsNC7zG233WaaN29uvvrqK7Ny5UrTq1cv06tXLz9WHTgqn1VkDG1dV5YvX25cLpd59NFHzU8//WRef/11ExYWZv773/96l3niiSdMTEyM+fDDD80PP/xghg4dyim6J2DMmDGmSZMm3tOh33//fdOwYUNzzz33eJehrU9MXl6e+f777833339vJJlnn33WfP/992b79u3GmNq168CBA03Xrl3NsmXLzDfffGOSkpI4Hfp0ev75503z5s1NcHCw6dGjh/nuu+/8XZLtSarxNnPmTO8yBw8eNHfccYepX7++CQsLM8OGDTO7d+/2X9EBpGpwoa3rzv/+9z9z7rnnmpCQENO+fXvzf//3fz7zPR6PmTJliomLizMhISGmf//+ZtOmTX6q1r5yc3PNXXfdZZo3b25CQ0NN69atzeTJk01xcbF3Gdr6xCxYsKDG/89jxowxxtSuXffv329GjhxpIiIiTFRUlBk3bpzJy8s76dosYypdYhAAAOAMxjEuAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANgguAADANv5/qMzKLhjXmLgAAAAASUVORK5CYII="},"metadata":{}}]},{"cell_type":"code","source":"#model = best_model     # least validation does not work well","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the training and test sets\nrunning_test_loss = 0.0\nfor xx,yy,tt  in test_loader:\n    xx = xx.to(device)\n    yy = yy.to(device)\n    tt = tt.to(device)\n    with torch.no_grad():\n        outputs = model(xx, tt*0.)    \n        test_loss = criterion(outputs, yy)\n    running_test_loss += test_loss.item()\n    \n# Calculate the MSE test sets\ntest_mse = running_test_loss/len(test_loader)\nprint(f\"Test MSE: {test_mse:.4f}\")\n\n# Plot predictions for a few samples from the test set\nsample = 0\nfor xx,yy,tt  in test_loader:\n    sample = sample + 1\n    xx = xx.to(device)\n    yy = yy.to(device)\n    tt = tt.to(device)\n    with torch.no_grad():\n        outputs = model(xx, tt*0.)    \n    \n    fire_location_pred = outputs[0,0,2,:,:]\n    fire_location_pred = fire_location_pred.detach().cpu().numpy()\n    fire_location_true = yy[0,0,2,:,:]\n    fire_location_true = fire_location_true.detach().cpu().numpy()\n    print(\"sample: \", sample)\n    plt.imshow(fire_location_pred.T)\n    plt.colorbar(orientation='horizontal')\n    plt.title(\"Fire Location Prediction\")\n    plt.show()\n    plt.imshow(fire_location_true.T)\n    plt.colorbar(orientation='horizontal')\n    plt.title(\"Fire Location True\")\n    plt.show()\n    err = (fire_location_true.T - fire_location_pred.T)\n    plt.imshow(err)\n    plt.colorbar(orientation='horizontal')\n    plt.title(\"Fire Location Error\")\n    plt.show()\n        \n    if sample >= 5:\n        break\n\n# # Prepare submission file\n# y_preds = {test_df['id'][idx]: pred_test_grid[idx][5:25].flatten(order='C').astype(np.float32) for idx in range(len(test_df))}\n\n# df = pd.DataFrame.from_dict(y_preds, orient='index')\n# df['id'] = df.index\n# df = df.reset_index(drop=True)\n# cols = ['id'] + df.columns.tolist()[:-1]\n# df = df[cols]\n\n# df.to_csv('submission.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission generation \n# 20 autoregression steps for 27 test samples\ny_preds = {}\nids = []\nfor idx in range(len(test_df)):\n    theta, ustar, xi_f, uin, alpha, id = load_dataX(idx, test_df, 'test')\n    \n    theta = torch.Tensor(theta).unsqueeze(1)\n    ustar = torch.Tensor(ustar).unsqueeze(1)\n    xi_f = torch.Tensor(xi_f).unsqueeze(1)\n    \n    uin_tensor = torch.zeros_like(xi_f) + uin\n    alpha_tensor = torch.zeros_like(xi_f) + alpha\n    \n    TUXUA = torch.cat([theta,ustar,xi_f, uin_tensor, alpha_tensor], dim=1)\n    TUXUA = TUXUA.unsqueeze(0)\n    \n    xx = TUXUA[:,-history:,...]\n    xx = xx.to(device)\n    \n    fire_loc = []\n    for i in range(20):\n        with torch.no_grad():        \n            output = model(xx, torch.zeros(1).to(device)) \n            Temp = torch.cat([output,xx[:,-1:,-2:,:,:]], dim=2) \n            xx = torch.cat([xx[:,1:,:,:,:], Temp], dim=1)\n            fire_loc.append(output[0,:,2,:,:])\n    \n    fire_location20 = torch.cat(fire_loc, dim=0)\n    fire_location20 = fire_location20.detach().cpu().numpy().flatten(order='C').astype(np.float32)\n    \n    y_preds[id]= fire_location20\n    ids.append(id)\n\ndf = pd.DataFrame.from_dict(y_preds,orient='index')\ndf['id'] = ids\n#df.info()\n\n#move id to first column\ncols = df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndf = df[cols]\n#reset index\ndf = df.reset_index(drop=True)\n\n#df.head()\ndf.info()\ndf.to_csv('submission.csv',index=False)\nprint('Generated Submission file' )","metadata":{"execution":{"iopub.status.busy":"2024-10-17T23:51:13.440854Z","iopub.execute_input":"2024-10-17T23:51:13.441221Z","iopub.status.idle":"2024-10-17T23:51:50.117885Z","shell.execute_reply.started":"2024-10-17T23:51:13.441186Z","shell.execute_reply":"2024-10-17T23:51:50.116928Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 27 entries, 0 to 26\nColumns: 72321 entries, id to 72319\ndtypes: float32(72320), int64(1)\nmemory usage: 7.4 MB\nGenerated Submission file\n","output_type":"stream"}]}]}